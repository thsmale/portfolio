[
  {
    "path": "posts/2022-03-13-react/",
    "title": "Paint",
    "description": "This application is like Microsoft Paint except it is a web application so users can play even if they don't have a Window's machine. This was built using HTML, CSS, and React. React is a library for creating UI's in javascript developed by Meta, formally known as facebook.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-13",
    "categories": [],
    "contents": "\nClick the link below to play or read a brief introduction.Link to application\nDrawOnEm\nPoke fun at some of the most controversial public figures by drawing on them!\n\nThis is the screen as you open the application. Players can draw on the canvas or select an image and draw on top of the image!\n\nAre you heartbroken that Justin Beiber broke up with Selena Gomez or that he pissed in a bucket and screamed “F Bill Clinton”? Well here is your chance to get back at him, take your anger out in a non violent manner and explore your artisitc side.\n\nWant to draw for fun, or write silly text, draw using your mouse in the white canvas. Enter text then move it around the canvas using the arrows.\n\nIf you are curious to see who is selected the most to draw on, view the database which updates everytime someone selects an image to edit.\n\n\n\n",
    "preview": "posts/2022-03-13-react/example_images/load_screen.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-12-td/",
    "title": "Topological Data Analysis",
    "description": "Using techniques developed in topological data analysis to determine changes in weather patterns for Tahoe City, CA over the last 100 years. Implemented using libraries from C++ and python.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nTopological Data Analysis Final Project\nAn Analysis of Tahoe Climate Data\nThomas Smale, Josh Meadows, Alex Richards, David Coles\nMathematics\nCalifornia State University, Chico\nUnited States\n28 April 2021\nClick to view the pdf version generated with Latex\nIntroduction\nOne of the most pressing matters on any college campus is climate change. Climate change is an issue that affects everyone in this world. As humans have evolved scientists believe that human actions are harming the planet. This is causing more extreme weather that is disrupting many natural habitats. As a result there may be enormous loss of both animal and human life. As students of a California State University, we have grown up in this beautiful state and come to appreciate the ground underneath us. We are concerned that human induced changes to the climate may result in our beautiful state turning to a dry desert. Working together and combining our backgrounds in math and computer science that we have learned at Chico State, we are going to apply groundbreaking methods to study our climate. One of the most beautiful regions in California is Lake Tahoe. It offers year round outdoor fun and our state depends on it for many resources. Since it is such a special place, people care about it and would hate to see it be destroyed. At 6,000 feet of elevation in the winter time it is cold and snowy, but warms up in the summertime with warm California blue skies. This diverse climate gives us many different aspects to analyze.\nThe data comes from the National Oceanic and Atmospheric Administration also known as NOAA, a federal government organization that documents climate all over the country. Government workers have been collecting daily data about the weather at its station in Tahoe City since 1903. This gives us over a 100 years of reliable data to analyze. These data points are available in a csv file that is about 43000 lines long. The data is multi dimensional as it has fields such as temperature, amount of precipitation, and 15 different weather types. We will combine all of these dimensions into a point cloud to study its shape. We will then analyze the data using topological data analysis to study its qualitative features.\nTopological data analysis is a great way to deal with multi dimensional data that traditional methods struggle to extract value from. We will also use traditional methods such as scatter plots or line plots to visualize the data. This is to help with our understanding of the data set and confirm our conclusions. However, we expect that using topological data analysis will provide us with information about the data not seen in the traditional graphs. Using data analysis we will be able to withdraw trends about the data such as if the temperatures are increasing or the amount of yearly snow is steadily decreasing. From this we will learn more about how the climate has been changing in Tahoe, which may be a reflection of the weather patterns across California.\nThe amount of data being collected in the world is an unbelievably large amount that increases with each day. Data can consist of our network traffic, social media accounts, and grocery receipts. Processing all of this data is an extreme challenge and those who can make sense of it are rewarded. The market size for data science is increasing as companies look to gain a competitive edge by utilizing data to make better decisions. Data analysis is no easy feat as the size of the data, noisiness, dimensions, and incompleteness cause challenges. There are many different ways to analyze data but in the last 15 years topological data analysis has been recognized as useful in dealing with high dimensional complex data. Topological data analysis is a combination of algebraic topology, computational geometry, computer science, and more. It measures the qualitative features of data by computing the persistent homology which utilizes algebraic topology.\nTopological Data Analysis allows effective and thorough examinations of all this data. In the case of Tahoe weather data there are some severe issues that needed to be overcome to be able to properly analyze the data. One such problem is the data set was first started in 1903, over 118 years ago. This creates issues where there is occasionally incomplete data spanning large gaps in time. There are also new data types added over time, such as snow, minimum temperatures, and maximum temperatures. Not only do all those issues arise, but the scale of the data is a problem that is hard to overcome. With 40,000 lines contained in the CSV file, each containing one to six pieces of data, it is understandable that being able to quickly analyze the data is a problem. As interesting as it would be to apply all the data and use TDA to analyze it, the data set is not suited for analyzing in a realistic time frame simply due to lack of proper computing power for the job.\nThe were many mechanisms involved to solve the gaps in the lack of data as well as the scale of it. One such mechanisms was obtaining yearly averages. As each data point was a day, spanning over a hundred years it was much more feasible to receive yearly averages than trying to process everyday, as the computations for every piece of data would take far too long. To also remedy this we are hoping to leverage a software package called Dionysus. Dionysus is written by a student who studied under Gunnar Carlson, Edelsburg at Duke, and is currently at Lawrence Berkeley Laboratory. It is written in C++ with a python interface. C++ will give the library its speed, while python will provide a interface that is friendly to work with. As a whole this will make the process proceed at a much faster pace. Another software package that has promise is Gudhi. Gudhi, similar to Dionysus, is designed specifically to handle everything Topological Analysis related. It generates data sets and visuals based on the presented data with very little need for modification. This makes the proccess much safer in the event something was incorrectly done, which leaves the end results as more reliable.\nVietoris-Rips Complexes\nWe will be using Vietoris-Rips Complexes to topologically analyze our data. The Vietoris-Rips Complex is a simplicial complex, which means it is made from connecting points, edges, triangles, tetrahedra, pentachora, and so on together and it can have totally separate connected complexes. The complex is created by first taking a point cloud in \\(\\mathbb{R}^n\\), creating balls of equal radius centered at each of the points, and varying that radius. Each point in the point cloud gets a point in the complex, and whenever two balls intersect their points are connected by an edge. Then whenever three points are all connected to each other, the edges are filled in with a triangle and the same for four points all connected being filled in with a tetrahedra, and so on.\nOnce we have the complex, we analyze its homology. Without going into too much detail of how this is derived, we will examine how many different connected complexes there are and how long they last with the radius increasing and how many holes there are in the complex that are present for a sufficiently large range of radii. We will not be analyzing the higher dimensional analogues of holes however, as computing these would take too much time, they wouldn’t last too long anyway, and finally the data we will analyze will only have two dimensions, again out of computing power restrictions. Future projects may want to analyze these though, as with higher computing power those concerns could vanish.\nTo view the homology and how it changes based on the radius, we will use persistence diagrams and bar codes. Persistence diagrams plot each “feature\" as a point, which will be connected complexes, holes, or higher dimensional analogues of holes. The \\(x\\) axis is what radius a feature appears or is”born\" at, and the \\(y\\) axis is what radius it disappears or “dies\" at. Therefore, there are no points below the line \\(y=x\\), and the distance from the points to that line indicates how long the feature lasted which can tell us how significant it was. Barcodes are similar, the \\(x\\) axis is the radius and each feature gets a bar with one end being the radius when the feature appeared and the other end being the radius when it disappeared. The bars are then stacked together along the \\(y\\) axis and we order them by death, with the ones that died soonest appearing at the bottom. These give us similar information to the persistence diagram, but have some differences. How long a feature lasted is instead indicated by the length of the bar, and we can more easily see trends in when features die which is especially helpful for the connected complexes.\nAlgorithm\nTo start, we decided to calculate the yearly average of the maximum and minimum temperatures. In short, this was because computation time for all points was overwhelming, but this is discussed more in the Evaluation section. When calculating the yearly average temperatures, we excluded any dates that had minimum or maximum temperatures missing for simplicity. Here is our final implementation to create our yearly averages:\n\n    with open('report/tahoe_city.csv', newline='') as csvfile:\n        #Use DictReader so list doesn't contain header row \n        climatereader = csv.DictReader(csvfile)\n        average_max = 0\n        average_min = 0\n        # keep track of number of dates in year to take average\n        num_dates_in_year = 0\n\n        for i,row in enumerate(climatereader):\n            if row['TMIN'] != \"\" and row['TMAX'] != \"\":\n                # Another date counted for the current year\n                num_dates_in_year += 1\n                average_min += string_to_float(row['TMIN'])\n                average_max += string_to_float(row['TMAX'])\n\n                # Get the year of the current row we are on. Date format is \n                # YYYY-MM-DD, so we split by \"-\" and get the 0th element(the year)\n                row_year = row['DATE'].split(\"-\")[0]\n                # If we are starting a new year\n                if row_year != cur_year :\n                    cur_year = row_year\n                    yearly_maxtemps.append(average_max / num_dates_in_year)\n                    yearly_mintemps.append(average_min / num_dates_in_year)\n                    num_dates_in_year = 0\n                    average_min = 0\n                    average_max = 0\n\nThe next task at hand was to create a point cloud from the data we had. It is important that our cloud had the correct dimensions that Dionysus expected. Because we had multiple one dimensional arrays, we had to combine them to make one large multidimensional array. There are many ways that one might do this, but we chose to use a convenient numpy function:\n\n    point_cloud = np.vstack((yearly_mintemps,yearly_maxtemps)).T\n\nAt this point, we have set up the point cloud, and are able to use Dionysus in order to calculate the Vietoris-Rips Complex from the data:\n\n    f = d.fill_rips(point_cloud, 3, 2)\n\nwhere 3 is the maximum number of dimensions, and 2 is the maximum radius of the balls. This function returns a filtration, which we then can use to get our persistence diagrams and barcodes:\n\n    p = d.homology_persistence(f)\n    dgms = d.init_diagrams(p,f)\n    d.plot.plot_diagram(dgms[0], show=True)\n    d.plot.plot_bars(dgms[0],show=True)\n\nwhere the index of dgms is the Betti number.\nThere were many different directions we could have went with this data set. We settled on having the minimum yearly average temperatures on the x-axis, and maximum yearly average temperatures on the y-axis. Sticking to two dimensions allows us to better visualize the data and verify that Dionysus is creating barcodes correctly.\nHere is a graph of the data:  And here is an example Vietoris-Rips Complex with a radius of 0.43:\nThe image was created using matplotlib by scattering the points, plotting circles with centers at each point, and then plotting lines if any two circles intersect. Two circles intersect if the distance between the two points is less than two times the radius. In the image, we can see that there are two prominent holes, which we expect to see in the barcodes and persistence diagrams for Betti 1.\nNow that we have a visualization of the Viertoris-Rips Complex, we can analyze the persistence diagrams and barcodes to see if the holes we observe above are reflected in the diagrams.\nTemperature Analysis\nHere are the persistence diagram and bar codes obtained from Dionysus:\nThe persistence diagram plots when a feature was created against when it vanished, if ever. In this persistence diagram, the features represented are connected complexes in blue and holes in orange, with the values being the radius of the Vietoris-Rips Complex at which they appear or disappear at. For a point, if those two values are very close together, that means the feature disappeared quickly after it appeared and thus isn’t as significant. In the diagram this can be seen as the distance from the point to the line.\nDue to the nature of the Vietoris-Rips Complex, all simplicial complexes start immediately as there is always a point at which none of the balls are touching and each ball contains only the point it is centered on. Then there cannot be any new complexes, as increasing the radius only causes the balls to begin to intersect and simplicial complexes to merge. This merging is what is shown in the persistence diagram as complexes vanishing.\nBecause of how the persistence diagram displays the connected complex data with lots of overlap in the points, it is harder to see any trends so we look at the barcodes instead. This shows that the radii at which complexes die are fairly evenly distributed. This shows that there was a wide variety of densities within the point cloud, with some points clustered together and others farther apart from other points. As there are a number of complexes surviving with higher radii, we can surmise that there are some outliers that are reasonably distant from the other points in the point cloud. We can also speculate that the more even distribution may be in part due to the yearly averages being taken instead of the full data set. Without that truncation of data, there could be more intermediate points that cause the complexes to collapse more quickly, while making the points that are outliers more clear.\nHoles in the data mean that there were spaces surrounded by points nearby each other that had no points in them. The shorter they last, the more likely they are to just be noise in the data caused by a random variation in the measurements that happens to result in a hole in the data. In the persistence diagram, we can see a few holes that may be significant. This means that the yearly averages do not completely fill the interior of the of the graph, leaving the larger holes.\nPrecipitation Analysis\nWith precipitation and snow we retrieved the values in a way similar to the previous data involving minimum and maximum temperatures. In this case we compared snowfall on the x-axis and precipitation on the y-axis for our visualizations. To come up with our data points we found the averages for each year, but unlike before, this data includes both precipitation and snowfall as long as one or the other has a valid piece of data. Otherwise empty data from both precipitation and snowfall is discarded.\nHaving two dimensional data for the rain and snow was found to be the best way to handle plotting our points. This also allows us to verify the results from Dionysus as well as Gudhi were presented without error. The generation of the point cloud allows us to learn more about the data we are working with. By using Gudhi, another software package, and Dionysus we were able to generate figures for both the persistence and bardcodes of this data set.\nPresented is the point cloud of this input data:\n\nYearly average point cloud\nHere we have the persistence data and the barcodes, again from Dionysus:\n\n\n\nWe will use the previous analysis to help us understand this new set of data. First, the holes last for very little time, as we see they are all clustered near the line in the persistence diagram and their bars are all very short. This means the data largely saturates the area it takes up, with no large spaces with no points that would create holes.\nSecond, we notice in the bar codes for connected complexes that most of the complexes merge quite quickly. This tells us that much of the data is clustered quite close together. After most of them merge, there are a small number of complexes which take a lot longer to merge. This means there are a number of outliers in the data, which we can clearly see in the point cloud.\nConclusion\nWe attempted to analyze our climate data using topological data analysis, a field that has grown in popularity over the years. We originally hoped to potentially gain new insights on the data, but this became unrealistic as we lacked in the computational power to run topological data analysis algorithms on a data set this large. Because of this, we down-scaled tremendously, and decided to focus on two categories from our data, maximum and minimum temperatures. This was an unfortunate compromise, as we feel our results may have held more value if we were able to use more data points, or increase the number of dimensions. More computational power would have allowed us to explore a fuller range of possibilities with our data set.\nDespite these setbacks, we were able to analyze the data appropriately using topological data analysis, and ended up getting results that were easy to understand. For maximum and minimum temperatures, it was clear to see from our persistence diagrams that there were two long-lasting holes in our point cloud, that both lived and died at roughly the same time. While it is unclear whether these holes are significant, it is worthwhile to note their similar birth and death times, which may lend clues to the shape of the data. For the yearly snowfall and rainfall values, we found that much of the data was clustered tightly together due to a lack of longer-living holes and that most of the connected complexes merged together quickly. There were also a number of complexes that took longer to merge together, meaning there were a few years that were outliers from the bulk of the data.\nThere are a lot of interesting insights that could be made from this data. In the future, it would be worthwhile to explore the relationship between the different categories of data, potentially in more than two dimensions. It is clear that the results of the topological data analysis tools we used were consistent with the visualizations of the point cloud we made in two dimensions. This is promising, and gives us confidence to move to higher dimensions, where visualizing the Vietoris-Rips Complex is no longer an option.\nWe also have created usable skeleton code for reading in data sets, translating them into meaningful data structures, and utilizing Dionysus in order to get the persistence diagrams and barcode. On top of this, we created a way to animate the Vietoris-Rips Complex as the radius grew. This will be applicable to other data sets as well, when considering whether or not they would be a good candidate for topological data analysis.\nOverall, we gained new insight on topological data analysis, and how it can be applied not only to our specific data set, but any general data set. While the persistence diagrams can be a bit more confusing than traditional data analysis methods, it is undeniable that there is significance in them. While topological data analysis might not be the best option for certain data sets, it is a worthy option to consider. We look forward to using these newfound skills on future data sets to try and extract meaningful information.\nReferences\nOtter, N., Porter, M.A., Tillmann, U. et al. A roadmap for the computation of persistent homology. EPJ Data Sci. 6, 17 (2017). https://doi.org/10.1140/epjds/s13688-017-0109\nJ. D. Hunter, “Matplotlib: A 2D Graphics Environment\", Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, 2007.\nPython Library Dionysus by Dmitriy Morozov\nGUDHI User and Reference Manual, The GUDHI Project, GUDHI Editorial Board, 3.4.1, 2021, “https://gudhi.inria.fr/doc/3.4.1/\\”\nPast Weather by Zip Code - Data Table. Past Weather by Zip Code - Data Table | NOAA Climate.gov. (n.d.). https://www.climate.gov/maps-data/dataset/past-weather-zip-code-data-table.\nEvaluation\nWe encountered several challenges during this project. To start, documentation for Dionysus is fairly sparse, and unfinished. It was unclear what format Dionysus expected the point cloud to be in, the syntax of the filtration returned by the Vietoris-Rips function, and what to do with the filtration once we had it. Another struggle we encountered was the amount of computation time that calculating the Vietoris-Rips Complex took. At first, it was very hard to pinpoint the issue, because the program would never come to completion. Even after cutting our data size in half, it would still run for an indefinite time, with one notable run taking over 2 hours before it was force-quit. In an effort to see if the program would ever complete, we used around 100 points, which would take at most a minute to two minutes. We decided this was a range that we felt comfortable with, and decided to instead take the yearly averages of the data, in order to decrease the size of the point cloud.\nTo download the csv files from NOAA one must fill out a request with the desired fields. There were limitations to the data requests, like the file size must be under a gigabyte. So the fields and years were strategically chosen to be just less than a gigabyte. Some requests took a few days to be fulfilled when it was supposed to take no longer than 24 hours. All of these values were inputted as strings even though the values for fields like temperature were integers. So each row that was read in needed to be casted to its appropriate value like an integer or floating point value. There are many ways to read in a csv file in python, but the most elegant was the dictionary reader because it did not include the first row which is the title of all the columns.\nThere were also several elements of the project that went particularly well. We were able to plot the data with ease, and even made an animation of the Vietoris-Rips complex with an increasing radius. This helped us visualize the data and try and make sense of our diagrams and barcodes. Another method we used the visualize the data was using Matplotlib. This was a new library which required some getting used to. The input for these graphs are one dimensional numpy arrays which were different than our multi dimensional arrays. Our scatter plots also had too many points for Matplotlib to display in a readable format. This required breaking the problem down into smaller ones by decreasing the data set without compromising its integrity. It became a very powerful tool as we learned how to customize our graphs by manipulating their properties like size, axes, and style. As we became more used with Matplotlib we were able to utilize it to visualize graphs to see what would be most useful to apply Dionysus too.\nThe four of us spent at least 20-30 hours on this project. A good amount of time was spent trying to work with our csv file and Dionysus, but the main portion of our time went to examining our results and trying to find meaning from them. We also found notable issues with applying the data to TDA. When handling the information, certain visualizations and graphs came out looking very clumped, which made the data much harder to truly understand. This mostly occurred when plotting the vietoris-rips where we had a large grouping of lines and bubbles that blended together. Overall these issues did not hinder the progress of the project. Throughout the course of the project we were all very self sufficient and were able to find resources for help if needed, but we mostly all helped one another meet goals.\n\n\n\n",
    "preview": "posts/2022-03-12-td/report/rips.jpg",
    "last_modified": "2022-04-21T14:48:12-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-checkers/",
    "title": "Checkers",
    "description": "A checkers game with a really powerful GUI using OpenGL. Test your skills against your friend or a bot trained using artificial intelligence and the C++ programming language.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-02",
    "categories": [],
    "contents": "\nChecker board\n There is an algorithm which determines what moves are permitted for each checker piece. The user can select any valid piece and move it to any valid square. When a hit is made the opposing piece is eliminated from the game. There is a mode where the user can play against an agent trained using artificial intelligence.\nHow it’s made - checker piece\n To make a checker piece one must draw a lot of triangles in a shape to resemble a circle. By adjusting the sine and cosine of 2/3rds of the coordinates we can closely resemble a circle with enough triangles. It also wouldn’t be possible without the help of radians.\nHow it’s made - checker board\n Everything in OpenGL is just a triangle. All the 3 coordinates for each triangle must be calculated manually. Along with an red, green, blue, alpha value in the range 0-1 for each corresponding (X, Y) coordinate. Then a buffer is binded to OpenGL and a call to the glDrawArrays() function is made. We specify the vertex shader than the OpenGL engine assembles the triangle shape, rasterizes it, fragments it, and blends it.\nArtifical Intelligence\nHere is a video of the two AI agents playing each other\nTo give the agents life, we implemented the alpha beta pruning algorithm. This gives the agent an advantage as they are able to look many moves ahead and decide the best move. As one can see from the end of the video, the agents figure out a way to keep playing forever therefore protecting themselves from being taken out. It should also be noted that the two agents do attempt the same moves every time they play each other. Also, another rule must be added to the game logic to protect against stalemates.\n\n\n\n",
    "preview": "posts/2022-03-02-checkers/board_state.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-firehistory/",
    "title": "California Fire History",
    "description": "Analyzing official California Fire History data collected by government agencies dating back to the late 1800's. This is done using the R programming language.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-02",
    "categories": [],
    "contents": "\nIntroduction\nCalifornia has had our largest fires in recorded history the last 2 years. Fires have had an effect on everyone in California weather it be due to air quality, or loss of property and loved ones. One school morning I woke up to a sky full of dark smoke, the smoke was so bad I could barely see down the block. Since that day in Chico there have been multiple other instances where smoke has blanketed the skies of California and ash has fallen from the skies. Smoke over the horizon is a terrible feeling as life as you know it can rapidly change without there being anything you can do about it. As a data science student, we are often just on our computer but we are learning complex skills which we can apply to the real world to make a difference. This project is a great opportunity to spend our time helping in the effort to solve current real world problems.\nGoal\nI will be exploring this fire perimeter dataset to learn more about what measures cal fire is taking to combat fires. I would like to see if I can tell if any of these measures have been successful or unsuccessful. In addition I would like to see what is causing these fires in California. I would like to see just how many acres are burning and what areas have been affected the most.\nMy initial data science questions are\nWhat were some of the biggest fires in California history?\nWhich agencies respond to the most fires?\nHow long does it take for fires to be contained?\nWhat causes the biggest fires?\nAre certain causes of fires becoming more or less frequent?\nCan the fire cause be predicted?\nIs there a relationship between year and fire size?\nAre no burn days effective in preventing human caused fires?\nCan we calculate what troops are the busiest?\nIs there a linear relationship between fire size and duration?\nAbout the dataset\nI am using the 2020 California Fire Perimeters data set available from gis.data.gov. A fire perimeter is the boundary of a fire measured by acres in this data set. This data set includes data from many different government agencies from different time periods. This data set includes prescribed burns and wildfire perimeters dating back to 1950 for CAL FIRE. Data from 2021 is not released until the fire season is over, which is on going at the time of this (Dec 18 2021). The United States Forest Service, USFS, has submitted records as far back as 1878 and the National Park Service, NPS, has submitted records from as far back as 1921. The types of fires included in this report has varied over the centuries which may add bias to the results. A couple important notes about the data is that from 1950 to 2001 it included USFS fires 10 acres and greater and CAL FIRE fires 300 acres and greater. BLM and NPS started inputting data since 2002 collecting fires 10 acres and greater. From 2002-2014 CAL FIRE expanded its criteria by including timber fires 10 acres or greater, brush fires 50 acres and greater, grass fires 300 acres and greater in size, wild land fires destroying 3 or more structures, and wild land fires causing 300,000 or more in damage. From 2014 and on the monetary requirement was dropped and the damage requirement is 3 or more habitable structures or commercial structures. In 1989 each unit was assigned to verify all 300 plus Acre fires from pre 1989 and as a result there is a statewide GIS layer from 1950-1999. Some errors that could occur when exploring this data is that duplicates may exist. For example, USFS and CAL FIRE could both capture the fire perimeter and submit it. In some cases they could even report different parameters of the same fire. While duplicate records is rare, there is an asterisk next to the cells that are the most accurate.\nExplanation of variables\nYEAR This set contains fire data from 1878 to 2020.\nSTATE While this data set is from CALFIRE there is some data from Oregon, Nevada, and Arizona. This data reports fires that occurred in both states, it does not specify the origin.\nAGENCY Different services may respond to the fires depending on jurisdiction. These services provide data to Cal Fire as a courtesy Different values we may see are CDF for California Department of Forestry and Fire Protection (Cal Fire), United States Forest Service (USFS), Bureau of Land Management (BLM), National Park Service (NPS), Contract Country (CC), Other FED (Federal Fire Protection Agency).\nUNIT_ID This is a series of digits to uniquely identify what units are responding to the fire.\nFIRE_NAME Fires are often named for geographic location or nearby landmarks like roads, lakes, rivers, and mountains.\nINC_NUM Number assigned by thee Emergency Command Center of the responsible agency for the fire.\nALARM_DATE The date the fire was brought to the attention of CALFIRE.\nCONT_DATE The date the fire was contained. A fire is 100% contained when a perimeter has been formed around the fire that will prevent it from spreading beyond the line. To form the perimeter fire fighters may use trenches (normally 10-12 feet and shallow), natural barriers like rivers, or even already burned patches of land. Once a fire is contained it may still be burning but within the perimeter.\nCAUSE An enumeration of values 1-19 for the reason the fire started. Enumeration 4 is campfire but enumeration 19 is illegal alien campfire which is confusing. Another confusing enumeration is 12 and 13 for firefighter training and non-firefighter training. Wouldn’t every fire that wasn’t firefighter training fall under this category of non-firefighter training. Interesting enumerations is 18 for escaped prescribed burn, 17 volcanic, 11 power line, 7 arson, 14 unknown, and 16 aircraft.\nCOMMENTS Miscellaneous comments that can provide more information about the fire.\nREPORT_AC Estimated area consumed in fire reported in acres.\nGIS_ACRES GIS is a geographic information system that uses information from satellites to make inferences. This is numerical data and the units are acres. Given the data is more complete for GIS_ACRES we will be working with this mostly.\nC_METHOD The method used to collect perimeter data. C stands for collection here. This is a range of digits from 1-8 that can be GPS, infrared, photos, hand drawn, or mixed collection methods.\nOBJECTIVE Either 1 suppression (wildfire) or 2 resource benefit (WFU). A WFU is allowing naturally ignited wild land fires like those started by lightning or lava to burn when in inaccessible terrain where people are not threatened. This is to avoid putting firefighters at risk and keep the land healthy.\nFIRE_NUM This has no description and is unclear at the moment. It is probably a method used to identify fires. There is not much research on it either, this will mostly be ignored.\nSHAPE_Length This is most likely GIS data. These map units are based on the coordinate system it could be square meters or something else. It could also be angular or linear.\nSHAPE_Area The units are unknown.\nDiscovery:\n\n[1] \"The fire dataset dimensions are (21318, 17)\"\n\n\n\nmissing_data <- c(1:ncol(fires))\nfor(col in 1:ncol(fires)) { \n  colname <- colnames(fires)[col]\n  nans <- sum(is.na(fires[, col]))\n  bad_strings <- c('', ' ', \"UNKNOWN\", \"UKNOWN\", \"N/A\")\n  bad_strings_count <- sum(fires[, col] %in% bad_strings)\n  zeros <- sum(fires[, col] <= 0)\n  total <- 0\n  if(!is.na(nans)) { \n    total <- nans \n  }\n  if(!is.na(bad_strings_count)) { \n    total <- total + bad_strings_count\n  }\n  if(!is.na(zeros)) { \n    total <- total + zeros\n  }\n  missing_data[col] <- total\n}\nmissing_data <- data.frame(colnames(fires), missing_data)\ncolnames(missing_data) <- c(\"Column\", \"Num NA's, '', or <= 0's\")\nmissing_data\n\n\n         Column Num NA's, '', or <= 0's\n1          year                      77\n2         state                       6\n3        agency                      10\n4       unit_id                      46\n5     fire_name                   13416\n6       inc_num                    1852\n7    alarm_date                   10728\n8     cont_date                   25360\n9         cause                      48\n10     comments                   36974\n11    report_ac                   12551\n12    gis_acres                       7\n13     c_method                   12222\n14    objective                     195\n15     fire_num                    8123\n16 shape_length                       0\n17   shape_area                   21317\n\nThere are many ways to deal with missing data like ignoring it or changing them to the mean or median. It is important to not change the integrity of the data if you manipulate missing data. The majority of the NA’s are in columns report_ac and c_method which means I will use shape_area instead. The rest of the values mostly come from missing values like ’’. This does not matter so much for comments but is a concern for inc_num, alarm_date, cont_data, and fire_num. For fire_name, many of the “UNKNOWN” fire names may stem from small prescribed burns. This is something to keep in mind as I use this data and will check to see if those old values originate from the older data or not.\nBrief summaries of every variable\nYEAR\n\n[1] \"The range of years in the dataset is from 1878 to 2020\"\n\n\nThis graph does not prove that more fires are occurring every year because we do not know how accurate reporting is in the early 1900s. CAL FIRE has data set going back to 1950 but USFS has data from 1878 in here. The two years with the most amount of data is 2017 and 2020 which has had the worse fires in recent history.\nSTATE\n\nTable 1: Number of observations for each state\nstates\nFreq\nAZ\n1\nCA\n21226\nNV\n73\nOR\n15\n\nNo fires that happened out of this state were under jurisdiction of Cal Fire which suggests these fires may have originated outside of California. These fires can be potentially misleading since the boundary can include both California and the other state. There is no way to determine what the boundary is for the California part of the fire.\nAGENCY\n\nTable 2: Number of reports by each agency\nagency\nnum_fires\nUSF\n9556\nCDF\n6563\nCCO\n3222\nNPS\n1011\nBLM\n652\nDOD\n163\nLRA\n111\nFWS\n22\nBIA\n12\nOTH\n1\n\nSurprisingly the United States Forest Service has been in charge of more fires than CalFire. However, this is likely due to them reporting many smaller prescribed burns. I wonder who is in charge of more land, and how jurisdiction is delegated. Private (PVT) was an option in the official documentation but there appears to be no occurrences in this data set. It seems like the groups to pay the most attention to are BLM, CCO, CDF, LRA, NPS, and USF.\n\n\n\n\n\n\nWe continue to see that the National Park Service has jurisdiction over more large fires than CAL Fire. Despite the increase of fires in recent years, the number of contracted counties in charge of fires has decreased. However, the Bureau of Land Management and National Park Service have been increasing their aid in combating California Fires.\nUNIT_ID\n\n\nout_of_state_units <- fires %>% \n  select(state, unit_id) %>% \n  filter(state != \"CA\" & state != \"\" & !is.na(state) & unit_id != '') %>% \n  unique()\nin_state_units <- fires %>% \n  select(state, unit_id) %>% \n  filter(state == \"CA\" & state != '' & !is.na(state) & unit_id != '') %>%\n  unique()\n#See if there are any units belonging to both states\ncross_state_units <- c() \nfor(row in 1:nrow(in_state_units)) { \n  unit <- in_state_units[row, \"unit_id\"]\n  if(sum(out_of_state_units[, \"unit_id\"] == unit) > 0) { \n    cross_state_units <- c(cross_state_units, unit)\n    }\n}\ncross_state_units_df <- fires %>% \n  filter(unit_id == cross_state_units & state != '' & !is.na(state)) %>% \n  select(state, agency, unit_id, gis_acres, fire_name) %>% \n  group_by(state, agency, unit_id) %>% \n  summarise(gis_acres_total = sum(gis_acres)) %>% \n  arrange(unit_id)\nkable(cross_state_units_df, caption = \n        \"The following units were responsible for fires in multiple states\")\n\n\nTable 3: The following units were responsible for fires in multiple states\nstate\nagency\nunit_id\ngis_acres_total\nCA\nBLM\nCCD\n1392.92076\nNV\nBLM\nCCD\n25.46643\nCA\nUSF\nHTF\n26751.67800\nNV\nUSF\nHTF\n14267.09300\nCA\nUSF\nKNF\n235463.13349\nOR\nUSF\nKNF\n476.97339\nCA\nBLM\nNOD\n18492.59674\nNV\nBLM\nNOD\n17.40460\nCA\nUSF\nTMU\n3092.50918\nCA\nUSF\nTOI\n18235.83105\nNV\nUSF\nTOI\n10366.60557\n\nThe reason that there are some units in charge of fires in multiple states is because they belong to federal organizations like United States Forest Service or Bureau of Land Management.\n\n\ncalfire_units_biggest_fire <- fires %>% \n  filter(agency == \"CDF\" & !is.na(gis_acres)) %>% \n  group_by(unit_id) %>% \n  slice(which.max(gis_acres)) %>% \n  select(year, agency, unit_id, fire_name, gis_acres) %>% \n  arrange(desc(gis_acres)) %>% \n  head(n=10)\nkable(calfire_units_biggest_fire, caption=\"CAL FIRE Units that Responded to the Top 10 Biggest Fires in California History\")\n\n\nTable 4: CAL FIRE Units that Responded to the Top 10 Biggest Fires in California History\nyear\nagency\nunit_id\nfire_name\ngis_acres\n2018\nCDF\nLNU\nRANCH\n410202.47\n2020\nCDF\nSCU\nSCU COMPLEX\n396399.00\n2007\nCDF\nSBC\nZACA\n240358.70\n2018\nCDF\nSHU\nCARR\n229651.41\n1977\nCDF\nBEU\nMARBLE-CONE\n173333.45\n2007\nCDF\nMVU\nWITCH\n162070.47\n2018\nCDF\nBTU\nCAMP\n153335.56\n1990\nCDF\nTGU\nCAMPBELL\n131504.22\n2020\nCDF\nCZU\nCZU LIGHTNING COMPLEX\n86553.46\n1985\nCDF\nSLU\nLAS PILITAS\n84271.42\n\nI would like to calculate the busiest years for each unit, however an accurate way of calculating this is stopping me. Factors to consider are center, spread, range, count, and max fires for each unit every year. I need a function that will return the hardest year based off this data.\nFIRE_NAME\n\n\n#There's still other bad fire names in the data set\nbad_fire_names <- c(\"UNKNOWN\", \"N/A\", \"\", \"UKNOWN\", \" \")\npopular_fire_names <- fires %>%\n  filter(!is.na(fire_name)) %>% \n  subset(!(fire_name %in% bad_fire_names)) %>% \n  group_by(fire_name) %>% \n  count(name=\"count\") %>% \n  arrange(desc(count)) %>% \n  head(n=10)\nkable(popular_fire_names, caption=\"10 Most Popular Names for Fires\")\n\n\nTable 5: 10 Most Popular Names for Fires\nfire_name\ncount\nCANYON\n45\nRIVER\n39\nLAKE\n37\nCREEK\n35\nCOTTONWOOD\n33\nBEAR\n32\nRIDGE\n30\nRANCH\n28\nSPRING\n28\nPINE\n27\n\n\n\nbiggest_fire_names <- fires %>% \n  arrange(desc(gis_acres)) %>% \n  select(year, fire_name, gis_acres) %>% \n  head(n=5)\nkable(biggest_fire_names, caption=\"Top 5 Biggest Fires in California History and their Names\")\n\n\nTable 6: Top 5 Biggest Fires in California History and their Names\nyear\nfire_name\ngis_acres\n2020\nAUGUST COMPLEX FIRES\n1032699.0\n2002\nBISCUIT\n501082.0\n2018\nRANCH\n410202.5\n2020\nSCU COMPLEX\n396399.0\n2020\nCREEK\n379882.2\n\n\n[1] \"A moment of condolences for anyone affected by the Camp Fire in Paradise or any other fire.\"\n  fire_name year unit_id gis_acres alarm_date  cont_date\n1      CAMP 2018     BTU  153335.6 2018-11-08 2018-11-26\n\nFire names are not unique and there are some fires that share the same name. There are many fire names that have been entered in incorrectly, that also contain back slashes or other gibberish.\nALARM_DATE\n\n\nnew_dates <- fires\nnew_dates$alarm_date <- as.Date(new_dates$alarm_date, \"%Y/%m/%d\")\nnew_dates$cont_date <- as.Date(new_dates$cont_date, \"%Y/%m/%d\")\nbusiest_days <- new_dates %>% \n  filter(!is.na(alarm_date)) %>% \n  group_by(alarm_date) %>% \n  summarise(num_fires = n(), \n            mean = mean(gis_acres), \n            median = median(gis_acres), \n            sd=sd(gis_acres)) %>% \n  arrange(desc(num_fires)) %>% \n  head(n=5)\nkable(busiest_days, caption=\"Top 5 Number of Daily Fires Since 1878\")\n\n\nTable 7: Top 5 Number of Daily Fires Since 1878\nalarm_date\nnum_fires\nmean\nmedian\nsd\n2008-06-21\n122\n5178.631\n245.33860\n16386.395\n1987-08-30\n61\n8144.967\n1932.24430\n13913.601\n2003-09-03\n56\n515.189\n58.68177\n1629.138\n2015-07-30\n50\n4250.587\n389.58737\n10438.913\n1999-08-23\n47\n7053.712\n1571.95800\n19095.666\n\nIt is hard to imagine that 122 fires are logged on June 6 even if it is a summer day. Before I analyze more time’s, lets find out the meaning behind this. One explanation is it could be a bunch of controlled burns. Many of these did share the same inc_number. However, many had different names and containment dates. While the primary cause on July 30, 2015 for lots of fires was smoking, on July 6, 2008 it was just lots of lightning.\nCONT_DATE\n\n\nformat <- \"%Y/%m/%d %H:%M:%S %z\"\nlongest_fires <- new_dates %>% \n  select(fire_name, alarm_date, cont_date, cause, objective) %>% \n  filter(!is.na(alarm_date) & !is.na(cont_date)) %>% \n  mutate(fire_duration = \n           difftime(cont_date, alarm_date, format, units=\"days\")) %>% \n  arrange(desc(fire_duration)) %>% \n  head(n=10)\nkable(longest_fires, caption=\"Longest Lasting Fires in California History Since 1878\")\n\n\nTable 8: Longest Lasting Fires in California History Since 1878\nfire_name\nalarm_date\ncont_date\ncause\nobjective\nfire_duration\nPIUTE\n1990-08-13\n1999-09-14\n1\n1\n3319 days\nRALSTON\n2006-09-05\n2007-09-17\n9\n1\n377 days\nWESTLAKE\n2006-07-08\n2007-07-09\n2\n1\n366 days\nLAVAL\n2012-06-03\n2013-06-03\n14\n1\n365 days\nGORGE\n2018-08-19\n2019-08-19\n14\n1\n365 days\nPANTHER\n2013-05-01\n2013-12-09\n9\n1\n222 days\nPOWERHOUSE\n2013-05-30\n2013-12-18\n9\n1\n202 days\nLAKE\n2015-06-17\n2015-12-31\n9\n1\n197 days\nROSASCO\n2019-05-23\n2019-12-02\n9\n1\n193 days\nPONY\n2016-06-07\n2016-12-15\n1\n1\n191 days\n\nIn the data set there are some alarm_dates and cont_dates entered incorrectly, like the cont_date occurring before the alarm_date. Hence, there is a possibility that these results have also been entered in incorrectly. The reason that some of these fires last so long, is that fire responders allow it, because it is in a rural area and fires can be beneficial. The cause for most of these fires is lightning, unknown, or miscellaneous.\n\n[1] “Fire Duration (Hours) Summary Statistics” Min. 1st Qu. Median Mean 3rd Qu. Max. 24 24 72 368 288 8760\n\nA problem is that there are so many missing values in cont_date there are very few fires you can find the fire duration for relative to the whole data set. Among those available, there is a huge variance in the duration of fires. These shorter lasting fires are not just prescribed burns and have many different causes.\nCAUSE\n\nTable 9: Causes of the Biggest Fires on Average since 1878\ncause\nstring_cause\navg_size\ncount\n4\nCampfire\n3530.47878\n380\n1\nLightning\n2969.34811\n3454\n16\nAircraft\n2474.22459\n14\n9\nMiscellaneous\n2285.43451\n3379\n7\nArson\n2200.11588\n903\n10\nVehicle\n1534.41743\n454\n14\nUnknown/Unidentified\n1496.36036\n9541\n2\nEquipment Use\n1460.92934\n1246\n11\nPower Line\n1274.56765\n412\n6\nRailroad\n1169.29763\n80\n3\nSmoking\n1048.65940\n342\n13\nNon-Firefighter Training\n1013.68027\n11\n5\nDebris\n875.22631\n723\n8\nPlaying with Fire\n657.54545\n196\n18\nEscaped Prescribed Burn\n623.48734\n90\n12\nFirefighter Training\n594.01999\n5\n15\nStructure\n391.06574\n21\n19\nIllegal Alien Campfire\n83.46374\n17\n\nAs I previously mentioned, the reason some fires duration is so long is because they are in remote areas and allowed to continue to burn. Despite this, human induced fires like campfires, still produce bigger fires on average than nature induced fires like lightning. In the next sections we will explore if certain causes of fires are becoming more or less frequent.\nCOMMENTS\nThe max length of a comment is 260 characters, comments are longer but they are cut off by an * indicating there is more to that comment somewhere. By reading comments interesting ones to me were “The cause was target shooting”, “… Total Cost 18,600,600”, “Children playing with fire”, and names of people. Most of the comments in the data set are empty.\nGIS_ACRES\n\nTable 10: Highest Acres Burnage in California History since 1878\nyear\ngis_acres\nsquare_miles\n2020\n4159334.0\n6498.959\n2018\n1590431.2\n2485.049\n2017\n1424559.4\n2225.874\n2008\n1382462.2\n2160.097\n2007\n1040224.3\n1625.350\n2003\n970479.3\n1516.374\n2002\n963898.5\n1506.091\n1987\n862910.9\n1348.298\n2012\n847714.6\n1324.554\n1999\n801137.0\n1251.777\n\nFor reference 500,000 acres is equal to 780 square miles, a square mile being a square with each side being 1 mile in length!!! The size of Yosemite national park is 1169 square miles, and the size of New York is 302 square miles.\n\n[1] \"Number of fire reports where the difference between report_ac and gis_acres is greater than 100: 1250\"\n\nThere are many differences between REPORT_AC and GIS_ACRES. Many of the discrepancies come from the 2000s as well so it is not the fault of the older data. There is less missing data for GIS_ACRES so we will primarily be depending on this. There is the possibility of using REPORT_AC data if GIS_ACRES is missing and vice versa.\nC_METHOD\n\n\nget_mode <- function(vec) { \n  frequencies <- table(vec) %>% sort(decreasing=TRUE)\n  strtoi(names(frequencies)[1])\n} \n\nc_method_yr <- fires %>% \n  filter(!is.na(c_method) & c_method != 8) %>%\n  group_by(year) %>%\n  summarise(c_method = get_mode(c_method))\nyear <- fires %>% filter(year == \"2020\")\n\n\n\n\n\n#Collection methods \nfirst_yr_c_method <- fires %>% select(year, c_method) %>% \n  drop_na() %>% arrange(year) %>% head()\ntotal_c <- table(fires$c_method)\nlabels <- c(\"GPS Ground\", \"GPS Air\", \"Infared\", \"Other Imagery\", \n            \"Photo Interpretation\", \"Hand Drawn\", \"Mixed Collection Tools\",\n            \"Unknown\")\ndf <- data.frame(Method = labels, count = as.vector(total_c))\ndf <- arrange(df, desc(count))\n\n\n\n\n\n\n\n\n\nHere we can see the transition of the most frequently used collection method as time goes on.\nOBJECTIVE\n\nTable 11: Top 5 Biggest Wildfires in California since 1878\nyear\nfire_name\ngis_acres\ncause\nobjective\n2020\nAUGUST COMPLEX FIRES\n1032699.0\n1\n1\n2002\nBISCUIT\n501082.0\n1\n1\n2018\nRANCH\n410202.5\n14\n1\n2020\nSCU COMPLEX\n396399.0\n1\n1\n2020\nCREEK\n379882.2\n14\n1\nTable 11: Top 5 Biggest Prescribed Burns since 1878\nyear\nfire_name\ngis_acres\ncause\nobjective\n2008\nCLOVER\n15788.590\n1\n2\n2008\nTEHIPITE\n11648.092\n1\n2\n1977\nFERGUSON\n10421.790\n1\n2\n2005\nCOMB\n9756.467\n1\n2\n2010\nSHEEP COMPLEX\n9021.522\n1\n2\n\nAs we saw previously with the longest lasting fires in recent history most of them originated from wildfires. It is interesting that the cause of the biggest prescribed fires have all been lightning. In fact, there are no prescribed burns that do not originate from lightning in this data set that has GIS_ACRES data.\n\n\n#Are number of prescribed burns increasing? \nyearly_resource_burns <- resource %>% \n  filter(!is.na(gis_acres) & gis_acres > 0) %>% \n  select(year, gis_acres) %>% \n  group_by(year) %>% \n  summarise(num_fires = n(), total_acres_burned = sum(gis_acres)) \n\n\n\n\n\n\nThe number of acres burned in prescribed burns has never been lower until this last decade. At the same time, California has experienced some of the worst fires in the last decade. While correlation does not prove causation, poor fire management in the off season may explain the recent surge in fires.\nExploratory Data Analysis\n\n\n# Number of acres burned 2010-2020\ndecade_fires <- fires %>% select(year, gis_acres) %>%\n  filter(year >= \"2010\" & !is.na(gis_acres)) \ndecade_fires_sum <- decade_fires %>% group_by(year) %>% \n  summarise(gis_acres = sum(gis_acres))\ndecade_fires_sum$gis_acres <- as.integer(decade_fires_sum$gis_acres)\nggplot(decade_fires_sum, aes(x=year, y=gis_acres)) + \n  geom_bar(stat=\"identity\", fill=\"gray70\") + \n  geom_text(aes(label=gis_acres)) + \n  xlab(\"Years\") + ylab(\"Acres Burned\") + \n  ggtitle(\"Number of Acres Burned 2010-2020\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArson does not have much statistical analysis, it is just surprising how much land is burned every year due to this crime.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThe biggest discovery is that the number of prescribed burns has never been lower from 2010-2020 in recent California Fire History. This may be cause for why we have seen such a big spike in fires in recent history. Also, R’s ggplot makes it extremely difficult to customize your graph.\n\n\n\n",
    "preview": "posts/2022-03-02-firehistory/firehistory_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-scout/",
    "title": "Scout",
    "description": "A cross platform mobile app for iOS and Android devices. This project was built using the Flutter framework by Google. This is an application for ordering girl scout cookies and gives girl scouts a dashboad to control their inventory.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-02",
    "categories": [],
    "contents": "\nLaunch Screen\nScout logoCookie Menu\n A menu to browse cookies and add to your cart. When you are at the cart, you are can return to the menu and edit your order. Once you are finished with your order you are taken to a form to fill out cookie order information.\nOrder Form\n Information is verified to be correct like a valid phone number and email address. A drop down menu is expanded to select the proper state and only orders from the USA are currently accepted.\nDatabase\n This page fetches data from the database so the scout can view order information for their customers.\nStats\n Data visualizations to see trends in cookie sales and track inventory.\nEvents\n Scouts have the ability to create events so people can be notified when and where they are selling cookies.\n\n\n\n",
    "preview": "posts/2022-03-02-scout/menu.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-01-ios-dev/",
    "title": "iOS development",
    "description": "Swift is an open source programming language Apple uses for iOS, macOS, tvOS, and other devices. These examples are with UIKit a more mature and compatible framework than SwiftUI at the time of this writing (2022). This is programatically done in UIKit without the use of Storyboard. This approach was taken so that the app would run more efficiently and there would be more control over existing frameworks.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\nUsing Apple Maps\n The application incorporates Apple’s most up to date privacy guidelines. Certain features are available depending on the user’s comfort level providing location data. If the user allows location, the map will focus on their current location.\nLocation Search Controller\n The user can enter in a location to annotate the map. The search controller uses data from Apple’s MapKit to respond with real live locations data like address, lng/lat, and place name. The location data cheats and looks for locations nearest you.\nUIButton\n This is a password protected application for encrypting information on your device. Once unlocked with your password the user has a UICollectionView to store data.\nProgramming the constraints\n Proper construction of constraints allows your application to scale to smaller devices like the iPhone 4 and bigger devices like the iPhone 10. The code for specifying the constraints for these 9 buttons is much more efficient than using storyboard for all 9 UIButtons.\nUITableViewController\n A UITableViewController is one of the most frequently used ViewControllers alongise the ever more popular UICollectionView. This is a scrollable feed where user’s can see their friends posts.\nProgramming a UITableViewController\n It is not that much code to programatically create a UITableViewController. However, there is another class for configuring the cell. The UITableViewCell class is a template that contains a UIImage and UITextView, but can be customized further.\n\n\n\n",
    "preview": "posts/2022-01-01-ios-dev/swift_images/wrongpassword.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-01-find-the-giraffe/",
    "title": "Find the Giraffe",
    "description": "Refresh the page to play again.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  }
]
