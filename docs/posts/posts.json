[
  {
    "path": "posts/2025-04-11-custom-color-picker/",
    "title": "Custom Color Picker",
    "description": "A custom color picker I implemented for MineChess to dynamically change the colors of the chess board.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": "https://minechess.vercel.app"
      }
    ],
    "date": "2025-04-11",
    "categories": [],
    "contents": "\nIntro\nMinechess is chess with a twist. Each turn, players are able to place a mine at one of the open squares. The location of this mine is hidden to the opponent. If the opponent moves their piece to the location where the mine was placed, their piece is eliminated or removed from the board.\nMineChess URL\nCustom Color Picker Source Code\nThis application was primarily developed by Ben Beisheim and Trevor Woon. The rest of this post will focus on my contribution to the application which is the color picker.\n\nDesign/Implementation\nInitially I was going to go with the html input element of type color. However, it was difficult to customize for an optimal user experience on MineChess. So I decided to leverage React Aria since these components aligned more with what I was trying to accomplish. React Aria was chosen over React Spectrum because it was more flexible in it’s customization options.\nOne of the main constraints I implemented is a restriction on color selection. Since it’s customary for a chess board to have a light and dark side to distinguish between players, users must choose a primary color using the hue slider, then adjust the light and dark variants from that base. This means players can’t make one side green and the other blue. While it’s technically possible to set both sides to the same color, it certainly isn’t recommended!\nUsage\nBelow is a mini interactive example of how the custom color picker works. Make sure to head over to MineChess and play your friends!\n\n\nNext steps\nA future feature to add to the custom color picker would be having the ability to save color combinations you have previously set. Players will be disappointed to find the color combination they created is no longer there when they reload the page. They may not remember what that exact color combo was and may always wish they could recreate that perfect color combo. This could be implemented by utilizing cookies or creating some backend service that stores previously set color combos for the player.\n\n\n\n",
    "preview": "posts/2025-04-11-custom-color-picker/images/game.png",
    "last_modified": "2025-04-12T00:04:01-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-04-10-greenlake-sustainability-dashboard/",
    "title": "Greenlake Sustainability Dashboard",
    "description": "A dashboard created for HPE Greenlake to reduce carbon emissions from a data center.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": "https://emeraldvue-5d3047c5816b.herokuapp.com"
      }
    ],
    "date": "2025-04-10",
    "categories": [],
    "contents": "\nIntro\nThis is a dashboard that enables users to reduce carbon emissions from a data center. There are processes in place that capture metrics from compute servers, networking equipment, or storage devices. The algorithm leverages this data to optimize resources, such as idling during periods of low usage, so the machines can have less carbon emissions. This helps save the planet while also reducing cost for data centers, it’s a win win!\nDashboard URL\nNote the URL may take a minute to load.\n\nSource Code\nUsage\nThe following is the view when you open the dashboard.\n\nThe algorithm will then show you the optimizations to compute resources or carbon emissions it is able to make. It will ask for confirmation before making the changes.\n\nDesign\nThe dashboard was built using the React component library Grommet. For UX inspiration the HPE design system was referenced. My friend and collegue Preston Massey-Blake made substantial contributions. The following is a high level overview of the architecture used to build this application.\nArchitecture\n\n\n",
    "preview": "posts/2025-04-10-greenlake-sustainability-dashboard/images/dashboard.png",
    "last_modified": "2025-04-10T22:53:43-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-03-04-claim3/",
    "title": "claim3",
    "description": "The claim3 platform aims to prevent unauthorized use and distribution of copyrighted content, providing content creators with a robust tool for protecting their intellectual property.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {
          "https://claim3.onrender.com": {}
        }
      }
    ],
    "date": "2024-03-04",
    "categories": [],
    "contents": "\nclaim3 is built with a wide variety of technologies including React, Django, Streamlit, PostgreSQL, and AWS.\n* Link to Application: https://claim3.onrender.com\nUsers can view a dashboard which shows analytics of content they have published across the internet.\nThey can filter their analytics based on a variety of social media platforms.\nDashboardIn addition, they can securely upload files which enables claim3 to track their content across the web.\nFile UploadPeople can view content uploaded to claim3 in a social media like feed.\nFeedOne can securely sign in or create an account as shown below.\nLoginSign Up\n\n\n",
    "preview": "posts/2024-03-04-claim3/images/dashboard.png",
    "last_modified": "2024-03-04T00:29:07-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-21-finfriends/",
    "title": "FinFriends",
    "description": "Fair finances among friends. A full stack web application to divide up bills from a trip amongst friends",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2024-01-21",
    "categories": [],
    "contents": "\n-> FinFriends URL\nFinFriends computes the least amount of transactions required among friends.\nIt then displays this data to the user.\nFin Friends UIThis web application is built using React.\nThe React component library being used is grommet.\nReact is a Javascript framework for programming front end web applications.\nThere is a backend API server written in the Python framework fastapi.\nThe front end sends the bills to the backend server which runs a python script\nto compute the least amount of Venmo’s required. It then returns these results\nto the user in the UI.\n-> FinFriends Source Code\n\n\n\n",
    "preview": "posts/2024-01-21-finfriends/fin-friends-ui-example.png",
    "last_modified": "2024-01-22T02:29:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-14-gifjifgame/",
    "title": "GifJifGame",
    "description": "Compete with your friends or strangers to see who can respond with the best gif to a given prompt. This is an ios game built with SwiftUI.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2023-01-14",
    "categories": [],
    "contents": "\nThe source code for GifJifGame can be found here on GitHub.\nHome ScreenHow to play\nIn GifJifGame the host decides a prompt. It is up to the everyone\nelse to respond with the best gif they can think of or find within the\ntime limit. The gifs are anonymously sent to the host where the host\nwill pick their favorite. The player who’s gif was chosen as the\nfavorite wins a point.\nSolo game play\n\n\n\n\n\n\nDon’t run out of time or this static snail will be submitted for you!\n\nGiphy\nThis game uses the Giphy API to provide users with access to the\nworld’s largest Gif database. Giphy also provides a SDK Swift\nprogrammers can incorporate via a package. The gif picker provided by\nGiphy is a UICollectionView from UIKit which is not directly accessible\nfrom SwiftUI. So to include it in your SwiftUI project it needs to be\nhacked by using a UIViewControllerRepresentable and UIViewRepresentable.\n\nInvitations\nPlayers are able to invite other plays to their game in between any\nrounds or when the game initially starts. To invite a player to the game\nenter their username. Players must have an account to receive an\ninvitation. Creating an account only requires a username and\npassword.\n\n\n\n\n\n\n\nIf users do not wish to create an account they can play in public\ngames. Public games are accessible to anyone with the app. In addition\nthere is a solo mode where prompts are randomly generated and players\ncan search giphy for an pertinent gif.\nStorage\nGifJifGame stores data in two ways, locally and remotely. This means\nif you change devices one can easily recover all their data because it\nalso resides in the database. User data is stored locally in the\nDocuments directory within the application sandbox. In the scenario\nbelow, I am on a new device so my data is fetched from the database.\n\nDatabase\nFirebase is used as the backend for this project. Each game is stored\nas a JSON document within a games collection. Each player has snapshots\nconfigured so documents can be read and written in real time. Snapshots\nsubscribe players to the games that they are in, so every time a change\nis made to that game document each player device receives a notification\nand their app is updated appropriately.\nThis is an example function of a user updating their username.\nFirebase is in a weird place at the moment, it supports async/await\nsyntax but there is more functionality available with closures at the\nmoment. This accepts a Player struct, then encodes it into a map that\nFirebase will accept. Once it makes a connection with the document in\nFirebase it will append the Player object to the array of Players\nalready in the game. The function will return true or false if the\noperation is successful.\nfunc add_player(doc_id: String, player: Player, completion: @escaping ((Bool) -> Void)) {\n    print(\"Adding player...\")\n    let encoded_player: [String: Any]\n    do {\n        // encode the swift struct instance into a dictionary\n        // using the Firestore encoder\n        encoded_player = try Firestore.Encoder().encode(player)\n    } catch {\n        // encoding error\n        print(\"Error encoding player \\(error)\")\n        completion(false)\n        return\n    }\n    let ref = db.collection(\"games\").document(doc_id)\n    ref.updateData([\n        \"players\": FieldValue.arrayUnion([encoded_player])\n    ]) { err in\n        if let err = err {\n            print(\"Error updating document: \\(err)\")\n            completion(false)\n        } else {\n            print(\"Document successfully updated\")\n            completion(true)\n        }\n    }\n}\nSwiftUI\nIn SwiftUI views are very cheap so code can be placed into structures\nwhich can easily be reused throughout the project. For example, this\nview is a search box that looks up other players. It is used within a\nfew different views in the project. It features a text field and when\nthe user presses enter it triggers a query to the database searching for\nthe username. While it is searching the database a spinning wheel will\nindicate to the user that the app is working on the task. A text box\nwill appear below the search box indicating the status of the\nsearch.\nstruct FetchUser: View {\n    @State private var username: String = \"\"\n    @State private var invalid_username = false\n    @State private var status = Status()\n    var action: (User) -> Void\n    \n    var body: some View {\n        HStack {\n            Image(systemName: \"magnifyingglass\")\n            TextField(\"Invite player via username\", text: $username, onEditingChanged: { _ in\n                invalid_username = false\n            })\n            .onSubmit {\n                if (username == \"\" || username.count >= MAX_USERNAME_LENGTH) {\n                    status.msg = \"Invalid username\"\n                    return\n                }\n                status.updating = true\n                get_user(username: username) { user in\n                    if let user = user {\n                        action(user)\n                    } else {\n                        status.msg = \"User does not exist\"\n                        invalid_username = true\n                    }\n                    status.updating = false\n                }\n            }\n        }\n        if (status.updating) {\n            ProgressView()\n        }\n        if(invalid_username) {\n            Text(status.msg)\n                .foregroundColor(.red)\n        }\n    }\n}\n\n\n\n",
    "preview": "posts/2023-01-14-gifjifgame/images/home.png",
    "last_modified": "2023-01-14T03:23:38-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-12-christmas-lights/",
    "title": "Christmas Lights",
    "description": "How LED strip lights can be hacked with an arduino and C code to have full control over the lights.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2023-01-12",
    "categories": [],
    "contents": "\nIntro\nIt was Christmas time and our house was looking rather dark. I had\nsome LED strip lights laying around with a remote. The problem was that\nthe remote was very limiting. The remote could turn the lights green or\nred, but not toggle between the two. Time to break out the Arduino and\nunleash the full power of these lights! Let there be light!\nBy using an Arduino we can control the current being sent to the LED\nlights by writing a C like program. By manipulating the current we can\nchange the RGB LED’s to any color of our choosing. By manipulating the\ntiming features of the Arduino through something such as Pulse Width\nModulation (PWM) we can get the LED’s to do some cool features as shown\nbelow.\nstrobe\n\n\n\n\n\n\n\nMaterials\nArduino UNO\n12V LED light strip\nMOSFET transistors\n10K ohm resistors\nInput/output barrel jack connectors\nJumper cables\nBreadboard or Prototype board\nSoldering iron and solder (optional)\nArduino IDE\nHardware\nThis will walk you through the process of turning these lights into\none’s that Santa will put you on the nice list for. These LED lights\nfeature 4 pins. Unfortunately, each LED is not individually\nprogrammable.\n The wires\nconnecting to the LED strip lights are set as follows (from top to\nbottom) Power, Green, Red, and Blue. Maybe I should have changed the\ncolor of power cable or red cable to not have 2 cables colored the same.\nHowever, I wanted to keep the classic power as red, and it wouldn’t have\nmade much sense to have a red cable colored yellow. So just be aware of\nthis moving forward.\nThe Arduino can only receive or transmit a max of 5 volts. The LED\nlights I had required 12 volts of power. If I were to send 12 volts\nthrough one of the Arduino pins it would fry the board. With how\nexpensive chips are right now, I didn’t want to do that. So the solution\nwas MOSFET transistors. These act as a gate between the 12V external\npower supply and the arduino. As long as the connection between the 12 V\nlights and 5V Arduino travels through the MOSFET, the Arduino will be\nsafe.\nThe MOSFET transistor has 3 pins, a gate, source, and drain (GSD).\nThe wire from the Arduino will go to the gate pin. The wire from the\nlight will go to the source pin. Lastly, the drain pin will be connected\nto ground. To explain in layman terms (as I’m no expert in this subject\nmatter), the gate pin will open and close the voltage flow to the\nArduino. When the voltage exceeds 5V it will redirect the excess voltage\nto the drain aka ground. All while maintaining the 12V needed to power\nthe LED light strip.\n The MOSFET\ntransistors have 3 legs, gate, source, and drain (from left to right).\nThere is a 10K ohm resistor between each Arduino pin connection and the\nMOSFET gate. This is to protect the Arduino from a discharge of current\nit cant handle. The middle pin (source) has a wire that connects to the\nLED pin. The third leg, (drain) connects to the ground. The power source\n(battery) is directly applied to the LED lights. The Arduino is powered\nby a separate, lower voltage supply such as a USB port on a computer.\nThe power source and Arduino are both connected to ground.\nThe breadboard is great for prototyping circuits quickly. However, it\nis not very sturdy as cables can easily be dislodged from the\nbreadboard. So for production, something more durable was necessary. So\nI upgraded to double sided protoboards. These allow for you to solder\nyour components so the connections can be more secure. They are a step\nabove breadboards but a step below Printed Circuit Boards (PCB).\n With some\nsoldering one can copy the breadboard circuit to the prototype board.\nKeep the breadboard circuit intact while you are making the protoboard\ncircuit. It is handy to use as a reference when something goes wrong\nsuch as a missing connection. Since the layout of the protoboard is\nslightly different, for example, no strips or rails I made a slightly\ndifferent circuit because it felt more organized.  The cables\nbranching out to the left connect to the Arduino. The cables branching\nout to the bottom connect to the LED lights.\n This shows components that are\nsoldered together. Each MOSFET leg is soldered to an adjacent pin which\ncontains a wire. The top row in the picture could all be soldered\ntogether as it is ground. Instead it is about 3 different ground wires\nsoldered together.\nThis is what everything looks like hooked up. The Arduino is\nconnected to the USB port of the computer. The barrel jack connector on\nthe protoboard is connected to a wall socket. \nSoftware\nThe code written to make these Christmas lights is publicly available\nvia GitHub.\nArduino UNO has a few pins that are pre programmed to use PWM so we\ndon’t have to make our own library to do it. The RGB pins are connected\nto PWM pins which are marked with a tilde ~ on the Arduino board. This\nis the code to achieve a fade effect on the lights using PWM.\n/*\n * pin: 3, 5, 6, 9, 10, 11 as they support PWM\n * time: (milliseconds) how long to wait until the brightness is increased \n * step: [0, 255] how much to increase the brightness by\n*/\nvoid brighten(int pin, int time, int step) {\n  int i = MIN; \n  while (i < 256) {\n    analogWrite(pin, i);\n    delay(time);\n    i += step;\n  }\n}\nOnce you can do a fade effect, you can also do the inverse brighten\neffect. By combining the two you can get a soothing red green pattern.\nThis is one of the modes and it ran for about 10 minutes long. The first\nminute it very slowly increases in color, increasing in speed each\niteration until 10 minutes has passed.\n//Speed of delay and step are changed\nvoid dynamic_fade() {\n  for (int delay = 100; delay < 1000; delay += 100) {\n    for (int step = 15; step <= 60; step += 15) {\n      brighten(red_pin, delay, step);\n      fade(red_pin, delay, step);\n      digitalWrite(red_pin, LOW); // some iterations red_pin doesn't reach 0\n      brighten(green_pin, delay, step);\n      fade(green_pin, delay, step);\n      digitalWrite(green_pin, LOW); // some iterations green_pin doesn't reach 0\n    }\n  }\n}\nBy setting the delay to a value of 100 milliseconds you can achieve a\nstrobe effect.\nint STROBE = 100; // Milliseconds\nwhile (1) {\n  analogWrite(red_pin, 255);\n  delay(STROBE);\n  digitalWrite(red_pin, LOW);\n  delay(STROBE);\n}\nRGB LED lights also open up much opportunity to customize the color.\nFor example, this was my favorite shade of orange.\nint MAX = 255;\nanalogWrite(red_pin, MAX);\nanalogWrite(green_pin, 1 * (MAX/12)); \nThe code runs the lights every day from 6 PM to 12 AM. Once the\nprogram has run for 6 hours, the Arduino goes to sleep for 21,600,000\nmilliseconds or until it is 6 PM the next day.\nThe code offers about 9 different Christmas light modes. It’ll switch\nbetween these modes every 10 minutes until it reaches 12 AM. My personal\nfavorite is the Fibonacci code from [0, 255]. Or the Christmas strobe\nlights (both featured above). On the list for next year, Merry Christmas\nflashing in morse code.\n\n\n\n",
    "preview": "posts/2023-01-12-christmas-lights/videos/fibonacci.gif",
    "last_modified": "2023-01-13T02:25:03-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-24-convolutions/",
    "title": "Convolutions",
    "description": "Applying convolutions to images using a gaussian filter to achieve a blurring/smoothing effect as seen in applications like adobe or instagram.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-05-24",
    "categories": [],
    "contents": "\nIntro\nComputer vision models like convolutional neural network’s use image processing techniques to extract the important features from an image. One of the first steps is smoothing or blurring the image. This makes the distribution of the pixels closer to normal, which is an assumption of many statistical methods. In this post we will look into how to achieve a blurring effect of images as seen in instagram or adobe photoshop.\nImages are represented in a computer using tensors or arrays. An image will be a matrix expressing its height, width, and number of components. The number of components determines if the image is colored or gray scale. Colored images may have 3 components, red, green, and blue. Gray scale images will have one component. The pixels in an image can range from [0, 1], [0, 255], or other options depending on it’s configuration. 0 will mean the pixel has no red at that element and the max value like 1 will mean it’s very red.\nRotating images\nWhen building a computer vision model images can be taken at all angles. It is possible for the model to misclassify an image due to the image being taken at a strange angle. So a flexible model will take these edge cases into account. Here we are reading in an image and formatting as a 3 dimensional array. There are three components for red, green, and blue. Each component is a 2 dimensional array with width 159 and height 240. By using techniques like the transpose of a matrix we are able to rotate the image.\n\n\n#Clockwise rotations\nimg <- readJPEG('images/mud.jpeg')\ndims <- dim(img)\nred <- img[,,1]\ngreen <- img[,,2]\nblue <- img[,,3]\n#90 degrees \n#Reverse order of rows, take transpose\nr <- t(red[nrow(red):1,])\ng <- t(green[nrow(green):1,])\nb <- t(blue[nrow(blue):1,])\nninety <- array(c(r, g, b), dim=c(dim(b), 3))\n#180 degrees \n#Reverse order of rows and cols\nr <- red[nrow(red):1, ncol(red):1]\ng <- green[nrow(green):1, ncol(green):1]\nb <- blue[nrow(blue):1, ncol(blue):1]\noneeighty <- array(c(r, g, b), dim=c(dim(b), 3))\n#270 degrees \n#Take transpose, reverse column order\nr <- t(red)[ncol(red):1,]\ng <- t(green)[ncol(green):1,]\nb <- t(blue)[ncol(blue):1,]\ntwoseventy <- array(c(r, g, b), dim=c(dim(b), 3))\n\n\n\n\n\n\nGaussian Smoothing\nA computer vision model tries different mathematical techniques to extract the most important information from an image. A common first step is smoothing the image which reduces it’s noise. One way to do this is to apply a Gaussian filter.\n\n\n\nThe distribution of this image is quite hectic so computer vision models will struggle to detect anything. There are many different peaks and skew.\n\n\n\nAfter applying a 2d convolution to the image using a gaussian filter, we can see the distribution os the image is much closer to normal!\n\n\n\n\n\n\n\n\n\nSmoothing the image is done by performing a 2d convolution. We set values in a 5x5 kernel using a Gaussian discrete approximation. We can see how adjusting the standard deviation of the Gaussian distribution affects the blur of the image. Next, we will explain what the Gaussian distribution is and later what a convolution is.\nGaussian Distribution aka Normal Distribution\nWhen working with images, we will use a 2 dimensional Gaussian function. \\[G(x, y) = \\frac{1}{2\\pi\\sigma^2}e^\\frac{x^2+y^2}{2\\sigma^2}\\]\n\n\ngaussian2d <- function(x, y) {\n  part1 <- 1 / (2*pi*sd(x*y)^2)\n  part2 <- exp(1)^((-(x^2+y^2))/(2*(sd(x*y)^2)))\n  return(part1*part2)\n}\n\n\n\n\n\n\nThis is what a 2 dimensional Gaussian function looks like. The Gaussian function is a continuous function. So we must fit it to a discrete approximation in order to apply it over the image. This is done by taking advantage of the fact that 1, 2, and 3 standard deviations away from the center of the bell curve contain 68%, 95%, and 99.7% of the values in the Gaussian distribution. So we can get a pretty accurate kernel with just 3x3 or 5x5 dimensions.\nConvolution\nConvolution is the process of applying the kernel over the original image. We place the anchor of kernel over every pixel in the image. Then perform a matrix multiplication with the kernel and the pixel’s neighbors. After that, compute the sum of the products. The output of this value is the new pixel value in the transformed image.\n2D convolution forumla\n\\[y[i, j] = \\sum_{m=-\\infty}^{\\infty} \\sum_{n=-\\infty}^{m=\\infty}h[m, n]*x[i-m,j-n]\\]\nHere x is the input image and y is the output which is the new image. H is the kernel matrix. I and j iterate over the image while m and n deal with that of the kernel.\nWhen applying a 2d convolution using a Gaussian filter it reduces the noise in the image. This gives the effect of blurring the image. One can do this in R using the following code.\n\n\n#Make a kernel of size 3x3\nx <- c(1, 0, 1, \n       1, 0, 1, \n       1, 0, 1)\ny <- c(1, 1, 1,\n       0, 0, 0, \n       1, 1, 1)\nsd <- 1.0\n#Fixed because we are setting the standard deviation\nkernel <- gaussian2d_fixed(x, y, sd)\n#Apply the convolution\nimg <- readJPEG('images/noisy_lady.jpeg')\nblur <- conv2(img, kernel, 'valid')\nblur <- array(blur, dim=c(dim(blur)))\n\n\n\n\n\n\nConclusion\nComputer vision depends on mathematical formulas that are commonly found in statistics as well. Understanding these statistical and mathematical formulas can give one a deeper understanding of what the computer vision model is doing. I hope you found this application of the Gaussian distribution interesting! Next, we will look at other distributions and see their effect on images.\n\n\n\n",
    "preview": "posts/2022-05-24-convolutions/convolutions_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-05-24T22:45:13-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-24-weathercollector/",
    "title": "Weather Collector",
    "description": "A web application that makes it easier for anyone to visualize and download over 1.4 billion rows of weather data from NOAA. This dashboard was created using the R programming language.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {
          "https://weathercollector.shinyapps.io/capstone/": {}
        }
      }
    ],
    "date": "2022-05-24",
    "categories": [],
    "contents": "\nIntro\nThis will take you through a run down of using this application to\ncompare weather data for a variety of different ski mountains. However,\nfeel free to use the application yourself by clicking this link.\nLink to Application: https://weathercollector.shinyapps.io/capstone/\nThe purpose of this application is to provide easier access to\npublicly available weather data. NOAA, under the U.S. Department of\nCommerce has 1.4 billion and counting rows of weather data available to\nthe public. However, accessing this data is very confined on it’s\nwebsite. This application provides a solution to this by working\ndirectly with the NOAA API and giving user’s an alternative user\ninterface.\nFinding Weather Stations\nHome ScreenThere are a variety of parameters to access NOAA stations. User’s\nattention will first be drawn to the map. Here user’s can view weather\nstations according to state and county. For more precise result’s user’s\ncan search locations in the search bar. This is hooked up to the Google\nplaces API so user’s can search places like Sun Valley ski resort. The\napplication will mark these locations with a red circle. User’s can\nclick red and blue circles, then apply the radius filter to get all\nstations within a certain amount of miles from the station.\nUnderneath the map the user is shown a list of stations present on\nthe map. The user is able to see a summary of the stations. The summary\nincludes information like the date range of data available for the\nstation and the different kind of data sets available. This allows the\nuser to be more informed about the kind of data the user can request. It\nis important for the user to know what data they are fetching because if\nthey are getting Daily Summaries for Park City Utah, they will be\nfetching over 200,000 observations.\nData TableHere the user has successfully fetched weather data for a few ski\nmountains. The user is presented with a table which they can use to\norganize the data using a few different parameters. The left show’s the\ndata they have fetched for, and modifying the contents of that panel\nwill affect what is shown on the table. The user can download the data\nto perform their own data analysis.\nExploratory Data AnalysisThe user can then perform exploratory data analysis to visualize the\ndata from within the application. They can use a variety of parameters\nto graph the data.\nEDA PanelThese values can be controlled to display what is graphed. For\nexample, if the user does not wish to graph data from Squaw Valley, they\ncan delete it then that data will not be displayed. There is also the\nability to adjust the X axis to control the number of year’s being\nplotted.\nThis web application was built entirely using the R programming\nlanguage. There is a framework called shiny that translates the R code\ninto HTML, bootstrap CSS, and Javascript. This allows anyone to access\nthe statistical powers of R. Shiny apps are able to be easily hosted\nusing shinyapps.io.\n\n\n\n",
    "preview": "posts/2022-05-24-weathercollector/images/homescreen.png",
    "last_modified": "2023-01-13T21:32:23-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-13-react/",
    "title": "Paint",
    "description": "This application is like Microsoft Paint except it is a web application so users can play even if they don't have a Window's machine. This was built using HTML, CSS, and React. React is a library for creating UI's in javascript developed by Meta, formally known as facebook.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-13",
    "categories": [],
    "contents": "\nClick the link below to play or read a brief introduction.Link to application\nDrawOnEm\nPoke fun at some of the most controversial public figures by drawing on them!\n\nThis is the screen as you open the application. Players can draw on the canvas or select an image and draw on top of the image!\n\nAre you heartbroken that Justin Beiber broke up with Selena Gomez or that he pissed in a bucket and screamed “F Bill Clinton”? Well here is your chance to get back at him, take your anger out in a non violent manner and explore your artisitc side.\n\nWant to draw for fun, or write silly text, draw using your mouse in the white canvas. Enter text then move it around the canvas using the arrows.\n\nIf you are curious to see who is selected the most to draw on, view the database which updates everytime someone selects an image to edit.\n\n\n\n",
    "preview": "posts/2022-03-13-react/example_images/load_screen.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-12-td/",
    "title": "Topological Data Analysis",
    "description": "Using techniques developed in topological data analysis to determine changes in weather patterns for Tahoe City, CA over the last 100 years. Implemented using libraries from C++ and python.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nTopological Data Analysis Final Project\nAn Analysis of Tahoe Climate Data\nThomas Smale, Josh Meadows, Alex Richards, David\nColes\nMathematics\nCalifornia State University, Chico\nUnited States\n28 April 2021\nClick\nto view the pdf version generated with Latex\nIntroduction\nOne of the most pressing matters on any college campus is climate\nchange. Climate change is an issue that affects everyone in this world.\nAs humans have evolved scientists believe that human actions are harming\nthe planet. This is causing more extreme weather that is disrupting many\nnatural habitats. As a result there may be enormous loss of both animal\nand human life. As students of a California State University, we have\ngrown up in this beautiful state and come to appreciate the ground\nunderneath us. We are concerned that human induced changes to the\nclimate may result in our beautiful state turning to a dry desert.\nWorking together and combining our backgrounds in math and computer\nscience that we have learned at Chico State, we are going to apply\ngroundbreaking methods to study our climate. One of the most beautiful\nregions in California is Lake Tahoe. It offers year round outdoor fun\nand our state depends on it for many resources. Since it is such a\nspecial place, people care about it and would hate to see it be\ndestroyed. At 6,000 feet of elevation in the winter time it is cold and\nsnowy, but warms up in the summertime with warm California blue skies.\nThis diverse climate gives us many different aspects to analyze.\nThe data comes from the National Oceanic and Atmospheric\nAdministration also known as NOAA, a federal government organization\nthat documents climate all over the country. Government workers have\nbeen collecting daily data about the weather at its station in Tahoe\nCity since 1903. This gives us over a 100 years of reliable data to\nanalyze. These data points are available in a csv file that is about\n43000 lines long. The data is multi dimensional as it has fields such as\ntemperature, amount of precipitation, and 15 different weather types. We\nwill combine all of these dimensions into a point cloud to study its\nshape. We will then analyze the data using topological data analysis to\nstudy its qualitative features.\nTopological data analysis is a great way to deal with multi\ndimensional data that traditional methods struggle to extract value\nfrom. We will also use traditional methods such as scatter plots or line\nplots to visualize the data. This is to help with our understanding of\nthe data set and confirm our conclusions. However, we expect that using\ntopological data analysis will provide us with information about the\ndata not seen in the traditional graphs. Using data analysis we will be\nable to withdraw trends about the data such as if the temperatures are\nincreasing or the amount of yearly snow is steadily decreasing. From\nthis we will learn more about how the climate has been changing in\nTahoe, which may be a reflection of the weather patterns across\nCalifornia.\nThe amount of data being collected in the world is an unbelievably\nlarge amount that increases with each day. Data can consist of our\nnetwork traffic, social media accounts, and grocery receipts. Processing\nall of this data is an extreme challenge and those who can make sense of\nit are rewarded. The market size for data science is increasing as\ncompanies look to gain a competitive edge by utilizing data to make\nbetter decisions. Data analysis is no easy feat as the size of the data,\nnoisiness, dimensions, and incompleteness cause challenges. There are\nmany different ways to analyze data but in the last 15 years topological\ndata analysis has been recognized as useful in dealing with high\ndimensional complex data. Topological data analysis is a combination of\nalgebraic topology, computational geometry, computer science, and more.\nIt measures the qualitative features of data by computing the persistent\nhomology which utilizes algebraic topology.\nTopological Data Analysis allows effective and thorough examinations\nof all this data. In the case of Tahoe weather data there are some\nsevere issues that needed to be overcome to be able to properly analyze\nthe data. One such problem is the data set was first started in 1903,\nover 118 years ago. This creates issues where there is occasionally\nincomplete data spanning large gaps in time. There are also new data\ntypes added over time, such as snow, minimum temperatures, and maximum\ntemperatures. Not only do all those issues arise, but the scale of the\ndata is a problem that is hard to overcome. With 40,000 lines contained\nin the CSV file, each containing one to six pieces of data, it is\nunderstandable that being able to quickly analyze the data is a problem.\nAs interesting as it would be to apply all the data and use TDA to\nanalyze it, the data set is not suited for analyzing in a realistic time\nframe simply due to lack of proper computing power for the job.\nThe were many mechanisms involved to solve the gaps in the lack of\ndata as well as the scale of it. One such mechanisms was obtaining\nyearly averages. As each data point was a day, spanning over a hundred\nyears it was much more feasible to receive yearly averages than trying\nto process everyday, as the computations for every piece of data would\ntake far too long. To also remedy this we are hoping to leverage a\nsoftware package called Dionysus. Dionysus is written by a student who\nstudied under Gunnar Carlson, Edelsburg at Duke, and is currently at\nLawrence Berkeley Laboratory. It is written in C++ with a python\ninterface. C++ will give the library its speed, while python will\nprovide a interface that is friendly to work with. As a whole this will\nmake the process proceed at a much faster pace. Another software package\nthat has promise is Gudhi. Gudhi, similar to Dionysus, is designed\nspecifically to handle everything Topological Analysis related. It\ngenerates data sets and visuals based on the presented data with very\nlittle need for modification. This makes the proccess much safer in the\nevent something was incorrectly done, which leaves the end results as\nmore reliable.\nVietoris-Rips\nComplexes\nWe will be using Vietoris-Rips Complexes to topologically analyze our\ndata. The Vietoris-Rips Complex is a simplicial complex, which means it\nis made from connecting points, edges, triangles, tetrahedra,\npentachora, and so on together and it can have totally separate\nconnected complexes. The complex is created by first taking a point\ncloud in \\(\\mathbb{R}^n\\), creating\nballs of equal radius centered at each of the points, and varying that\nradius. Each point in the point cloud gets a point in the complex, and\nwhenever two balls intersect their points are connected by an edge. Then\nwhenever three points are all connected to each other, the edges are\nfilled in with a triangle and the same for four points all connected\nbeing filled in with a tetrahedra, and so on.\nOnce we have the complex, we analyze its homology. Without going into\ntoo much detail of how this is derived, we will examine how many\ndifferent connected complexes there are and how long they last with the\nradius increasing and how many holes there are in the complex that are\npresent for a sufficiently large range of radii. We will not be\nanalyzing the higher dimensional analogues of holes however, as\ncomputing these would take too much time, they wouldn’t last too long\nanyway, and finally the data we will analyze will only have two\ndimensions, again out of computing power restrictions. Future projects\nmay want to analyze these though, as with higher computing power those\nconcerns could vanish.\nTo view the homology and how it changes based on the radius, we will\nuse persistence diagrams and bar codes. Persistence diagrams plot each\n“feature\" as a point, which will be connected complexes, holes, or\nhigher dimensional analogues of holes. The \\(x\\) axis is what radius a feature appears\nor is”born\" at, and the \\(y\\) axis is\nwhat radius it disappears or “dies\" at. Therefore, there are no points\nbelow the line \\(y=x\\), and the\ndistance from the points to that line indicates how long the feature\nlasted which can tell us how significant it was. Barcodes are similar,\nthe \\(x\\) axis is the radius and each\nfeature gets a bar with one end being the radius when the feature\nappeared and the other end being the radius when it disappeared. The\nbars are then stacked together along the \\(y\\) axis and we order them by death, with\nthe ones that died soonest appearing at the bottom. These give us\nsimilar information to the persistence diagram, but have some\ndifferences. How long a feature lasted is instead indicated by the\nlength of the bar, and we can more easily see trends in when features\ndie which is especially helpful for the connected complexes.\nAlgorithm\nTo start, we decided to calculate the yearly average of the maximum\nand minimum temperatures. In short, this was because computation time\nfor all points was overwhelming, but this is discussed more in the\nEvaluation section. When calculating the yearly average temperatures, we\nexcluded any dates that had minimum or maximum temperatures missing for\nsimplicity. Here is our final implementation to create our yearly\naverages:\n\n    with open('report/tahoe_city.csv', newline='') as csvfile:\n        #Use DictReader so list doesn't contain header row \n        climatereader = csv.DictReader(csvfile)\n        average_max = 0\n        average_min = 0\n        # keep track of number of dates in year to take average\n        num_dates_in_year = 0\n\n        for i,row in enumerate(climatereader):\n            if row['TMIN'] != \"\" and row['TMAX'] != \"\":\n                # Another date counted for the current year\n                num_dates_in_year += 1\n                average_min += string_to_float(row['TMIN'])\n                average_max += string_to_float(row['TMAX'])\n\n                # Get the year of the current row we are on. Date format is \n                # YYYY-MM-DD, so we split by \"-\" and get the 0th element(the year)\n                row_year = row['DATE'].split(\"-\")[0]\n                # If we are starting a new year\n                if row_year != cur_year :\n                    cur_year = row_year\n                    yearly_maxtemps.append(average_max / num_dates_in_year)\n                    yearly_mintemps.append(average_min / num_dates_in_year)\n                    num_dates_in_year = 0\n                    average_min = 0\n                    average_max = 0\n\nThe next task at hand was to create a point cloud from the data we\nhad. It is important that our cloud had the correct dimensions that\nDionysus expected. Because we had multiple one dimensional arrays, we\nhad to combine them to make one large multidimensional array. There are\nmany ways that one might do this, but we chose to use a convenient numpy\nfunction:\n\n    point_cloud = np.vstack((yearly_mintemps,yearly_maxtemps)).T\n\nAt this point, we have set up the point cloud, and are able to use\nDionysus in order to calculate the Vietoris-Rips Complex from the\ndata:\n\n    f = d.fill_rips(point_cloud, 3, 2)\n\nwhere 3 is the maximum number of dimensions, and 2 is the maximum\nradius of the balls. This function returns a filtration, which we then\ncan use to get our persistence diagrams and barcodes:\n\n    p = d.homology_persistence(f)\n    dgms = d.init_diagrams(p,f)\n    d.plot.plot_diagram(dgms[0], show=True)\n    d.plot.plot_bars(dgms[0],show=True)\n\nwhere the index of dgms is the Betti number.\nThere were many different directions we could have went with this\ndata set. We settled on having the minimum yearly average temperatures\non the x-axis, and maximum yearly average temperatures on the y-axis.\nSticking to two dimensions allows us to better visualize the data and\nverify that Dionysus is creating barcodes correctly.\nHere is a graph of the data:  And here is an example Vietoris-Rips Complex with a\nradius of 0.43:\nThe image was created using matplotlib by scattering the points,\nplotting circles with centers at each point, and then plotting lines if\nany two circles intersect. Two circles intersect if the distance between\nthe two points is less than two times the radius. In the image, we can\nsee that there are two prominent holes, which we expect to see in the\nbarcodes and persistence diagrams for Betti 1.\nNow that we have a visualization of the Viertoris-Rips Complex, we\ncan analyze the persistence diagrams and barcodes to see if the holes we\nobserve above are reflected in the diagrams.\nTemperature\nAnalysis\nHere are the persistence diagram and bar codes obtained from\nDionysus:\nThe persistence diagram plots when a feature was created against when it\nvanished, if ever. In this persistence diagram, the features represented\nare connected complexes in blue and holes in orange, with the values\nbeing the radius of the Vietoris-Rips Complex at which they appear or\ndisappear at. For a point, if those two values are very close together,\nthat means the feature disappeared quickly after it appeared and thus\nisn’t as significant. In the diagram this can be seen as the distance\nfrom the point to the line.\nDue to the nature of the Vietoris-Rips Complex, all simplicial\ncomplexes start immediately as there is always a point at which none of\nthe balls are touching and each ball contains only the point it is\ncentered on. Then there cannot be any new complexes, as increasing the\nradius only causes the balls to begin to intersect and simplicial\ncomplexes to merge. This merging is what is shown in the persistence\ndiagram as complexes vanishing.\nBecause of how the persistence diagram displays the connected complex\ndata with lots of overlap in the points, it is harder to see any trends\nso we look at the barcodes instead. This shows that the radii at which\ncomplexes die are fairly evenly distributed. This shows that there was a\nwide variety of densities within the point cloud, with some points\nclustered together and others farther apart from other points. As there\nare a number of complexes surviving with higher radii, we can surmise\nthat there are some outliers that are reasonably distant from the other\npoints in the point cloud. We can also speculate that the more even\ndistribution may be in part due to the yearly averages being taken\ninstead of the full data set. Without that truncation of data, there\ncould be more intermediate points that cause the complexes to collapse\nmore quickly, while making the points that are outliers more clear.\nHoles in the data mean that there were spaces surrounded by points\nnearby each other that had no points in them. The shorter they last, the\nmore likely they are to just be noise in the data caused by a random\nvariation in the measurements that happens to result in a hole in the\ndata. In the persistence diagram, we can see a few holes that may be\nsignificant. This means that the yearly averages do not completely fill\nthe interior of the of the graph, leaving the larger holes.\nPrecipitation\nAnalysis\nWith precipitation and snow we retrieved the values in a way similar\nto the previous data involving minimum and maximum temperatures. In this\ncase we compared snowfall on the x-axis and precipitation on the y-axis\nfor our visualizations. To come up with our data points we found the\naverages for each year, but unlike before, this data includes both\nprecipitation and snowfall as long as one or the other has a valid piece\nof data. Otherwise empty data from both precipitation and snowfall is\ndiscarded.\nHaving two dimensional data for the rain and snow was found to be the\nbest way to handle plotting our points. This also allows us to verify\nthe results from Dionysus as well as Gudhi were presented without error.\nThe generation of the point cloud allows us to learn more about the data\nwe are working with. By using Gudhi, another software package, and\nDionysus we were able to generate figures for both the persistence and\nbardcodes of this data set.\nPresented is the point cloud of this input data:\n\nYearly average point cloud\nHere we have the persistence data and the barcodes, again from\nDionysus:\n\n\n\nWe will use the previous analysis to help us understand this new set\nof data. First, the holes last for very little time, as we see they are\nall clustered near the line in the persistence diagram and their bars\nare all very short. This means the data largely saturates the area it\ntakes up, with no large spaces with no points that would create\nholes.\nSecond, we notice in the bar codes for connected complexes that most\nof the complexes merge quite quickly. This tells us that much of the\ndata is clustered quite close together. After most of them merge, there\nare a small number of complexes which take a lot longer to merge. This\nmeans there are a number of outliers in the data, which we can clearly\nsee in the point cloud.\nConclusion\nWe attempted to analyze our climate data using topological data\nanalysis, a field that has grown in popularity over the years. We\noriginally hoped to potentially gain new insights on the data, but this\nbecame unrealistic as we lacked in the computational power to run\ntopological data analysis algorithms on a data set this large. Because\nof this, we down-scaled tremendously, and decided to focus on two\ncategories from our data, maximum and minimum temperatures. This was an\nunfortunate compromise, as we feel our results may have held more value\nif we were able to use more data points, or increase the number of\ndimensions. More computational power would have allowed us to explore a\nfuller range of possibilities with our data set.\nDespite these setbacks, we were able to analyze the data\nappropriately using topological data analysis, and ended up getting\nresults that were easy to understand. For maximum and minimum\ntemperatures, it was clear to see from our persistence diagrams that\nthere were two long-lasting holes in our point cloud, that both lived\nand died at roughly the same time. While it is unclear whether these\nholes are significant, it is worthwhile to note their similar birth and\ndeath times, which may lend clues to the shape of the data. For the\nyearly snowfall and rainfall values, we found that much of the data was\nclustered tightly together due to a lack of longer-living holes and that\nmost of the connected complexes merged together quickly. There were also\na number of complexes that took longer to merge together, meaning there\nwere a few years that were outliers from the bulk of the data.\nThere are a lot of interesting insights that could be made from this\ndata. In the future, it would be worthwhile to explore the relationship\nbetween the different categories of data, potentially in more than two\ndimensions. It is clear that the results of the topological data\nanalysis tools we used were consistent with the visualizations of the\npoint cloud we made in two dimensions. This is promising, and gives us\nconfidence to move to higher dimensions, where visualizing the\nVietoris-Rips Complex is no longer an option.\nWe also have created usable skeleton code for reading in data sets,\ntranslating them into meaningful data structures, and utilizing Dionysus\nin order to get the persistence diagrams and barcode. On top of this, we\ncreated a way to animate the Vietoris-Rips Complex as the radius grew.\nThis will be applicable to other data sets as well, when considering\nwhether or not they would be a good candidate for topological data\nanalysis.\nOverall, we gained new insight on topological data analysis, and how\nit can be applied not only to our specific data set, but any general\ndata set. While the persistence diagrams can be a bit more confusing\nthan traditional data analysis methods, it is undeniable that there is\nsignificance in them. While topological data analysis might not be the\nbest option for certain data sets, it is a worthy option to consider. We\nlook forward to using these newfound skills on future data sets to try\nand extract meaningful information.\nReferences\nOtter, N., Porter, M.A., Tillmann, U. et al. A roadmap for the\ncomputation of persistent homology. EPJ Data Sci. 6, 17 (2017). https://doi.org/10.1140/epjds/s13688-017-0109\nJ. D. Hunter, “Matplotlib: A 2D Graphics Environment\", Computing in\nScience & Engineering, vol. 9, no. 3, pp. 90-95, 2007.\nPython Library Dionysus by Dmitriy Morozov\nGUDHI User and Reference Manual, The GUDHI Project, GUDHI Editorial\nBoard, 3.4.1, 2021, “https://gudhi.inria.fr/doc/3.4.1/\\”\nPast Weather by Zip Code - Data Table. Past Weather by Zip Code -\nData Table | NOAA Climate.gov. (n.d.). https://www.climate.gov/maps-data/dataset/past-weather-zip-code-data-table.\nEvaluation\nWe encountered several challenges during this project. To start,\ndocumentation for Dionysus is fairly sparse, and unfinished. It was\nunclear what format Dionysus expected the point cloud to be in, the\nsyntax of the filtration returned by the Vietoris-Rips function, and\nwhat to do with the filtration once we had it. Another struggle we\nencountered was the amount of computation time that calculating the\nVietoris-Rips Complex took. At first, it was very hard to pinpoint the\nissue, because the program would never come to completion. Even after\ncutting our data size in half, it would still run for an indefinite\ntime, with one notable run taking over 2 hours before it was force-quit.\nIn an effort to see if the program would ever complete, we used around\n100 points, which would take at most a minute to two minutes. We decided\nthis was a range that we felt comfortable with, and decided to instead\ntake the yearly averages of the data, in order to decrease the size of\nthe point cloud.\nTo download the csv files from NOAA one must fill out a request with\nthe desired fields. There were limitations to the data requests, like\nthe file size must be under a gigabyte. So the fields and years were\nstrategically chosen to be just less than a gigabyte. Some requests took\na few days to be fulfilled when it was supposed to take no longer than\n24 hours. All of these values were inputted as strings even though the\nvalues for fields like temperature were integers. So each row that was\nread in needed to be casted to its appropriate value like an integer or\nfloating point value. There are many ways to read in a csv file in\npython, but the most elegant was the dictionary reader because it did\nnot include the first row which is the title of all the columns.\nThere were also several elements of the project that went\nparticularly well. We were able to plot the data with ease, and even\nmade an animation of the Vietoris-Rips complex with an increasing\nradius. This helped us visualize the data and try and make sense of our\ndiagrams and barcodes. Another method we used the visualize the data was\nusing Matplotlib. This was a new library which required some getting\nused to. The input for these graphs are one dimensional numpy arrays\nwhich were different than our multi dimensional arrays. Our scatter\nplots also had too many points for Matplotlib to display in a readable\nformat. This required breaking the problem down into smaller ones by\ndecreasing the data set without compromising its integrity. It became a\nvery powerful tool as we learned how to customize our graphs by\nmanipulating their properties like size, axes, and style. As we became\nmore used with Matplotlib we were able to utilize it to visualize graphs\nto see what would be most useful to apply Dionysus too.\nThe four of us spent at least 20-30 hours on this project. A good\namount of time was spent trying to work with our csv file and Dionysus,\nbut the main portion of our time went to examining our results and\ntrying to find meaning from them. We also found notable issues with\napplying the data to TDA. When handling the information, certain\nvisualizations and graphs came out looking very clumped, which made the\ndata much harder to truly understand. This mostly occurred when plotting\nthe vietoris-rips where we had a large grouping of lines and bubbles\nthat blended together. Overall these issues did not hinder the progress\nof the project. Throughout the course of the project we were all very\nself sufficient and were able to find resources for help if needed, but\nwe mostly all helped one another meet goals.\n\n\n\n",
    "preview": "posts/2022-03-12-td/report/rips.jpg",
    "last_modified": "2023-01-14T02:57:14-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-checkers/",
    "title": "Checkers",
    "description": "A checkers game with a really powerful GUI using OpenGL. Test your skills against your friend or a bot trained using artificial intelligence and the C++ programming language.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-02",
    "categories": [],
    "contents": "\nChecker board\n There is an algorithm which determines what moves are permitted for each checker piece. The user can select any valid piece and move it to any valid square. When a hit is made the opposing piece is eliminated from the game. There is a mode where the user can play against an agent trained using artificial intelligence.\nHow it’s made - checker piece\n To make a checker piece one must draw a lot of triangles in a shape to resemble a circle. By adjusting the sine and cosine of 2/3rds of the coordinates we can closely resemble a circle with enough triangles. It also wouldn’t be possible without the help of radians.\nHow it’s made - checker board\n Everything in OpenGL is just a triangle. All the 3 coordinates for each triangle must be calculated manually. Along with an red, green, blue, alpha value in the range 0-1 for each corresponding (X, Y) coordinate. Then a buffer is binded to OpenGL and a call to the glDrawArrays() function is made. We specify the vertex shader than the OpenGL engine assembles the triangle shape, rasterizes it, fragments it, and blends it.\nArtifical Intelligence\nHere is a video of the two AI agents playing each other\nTo give the agents life, we implemented the alpha beta pruning algorithm. This gives the agent an advantage as they are able to look many moves ahead and decide the best move. As one can see from the end of the video, the agents figure out a way to keep playing forever therefore protecting themselves from being taken out. It should also be noted that the two agents do attempt the same moves every time they play each other. Also, another rule must be added to the game logic to protect against stalemates.\n\n\n\n",
    "preview": "posts/2022-03-02-checkers/board_state.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-firehistory/",
    "title": "California Fire History",
    "description": "Analyzing official California Fire History data collected by government agencies dating back to the late 1800's. This is done using the R programming language.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-02",
    "categories": [],
    "contents": "\nIntroduction\nCalifornia has had our largest fires in recorded history the last 2 years. Fires have had an effect on everyone in California weather it be due to air quality, or loss of property and loved ones. One school morning I woke up to a sky full of dark smoke, the smoke was so bad I could barely see down the block. Since that day in Chico there have been multiple other instances where smoke has blanketed the skies of California and ash has fallen from the skies. Smoke over the horizon is a terrible feeling as life as you know it can rapidly change without there being anything you can do about it. As a data science student, we are often just on our computer but we are learning complex skills which we can apply to the real world to make a difference. This project is a great opportunity to spend our time helping in the effort to solve current real world problems.\nGoal\nI will be exploring this fire perimeter dataset to learn more about what measures cal fire is taking to combat fires. I would like to see if I can tell if any of these measures have been successful or unsuccessful. In addition I would like to see what is causing these fires in California. I would like to see just how many acres are burning and what areas have been affected the most.\nMy initial data science questions are\nWhat were some of the biggest fires in California history?\nWhich agencies respond to the most fires?\nHow long does it take for fires to be contained?\nWhat causes the biggest fires?\nAre certain causes of fires becoming more or less frequent?\nCan the fire cause be predicted?\nIs there a relationship between year and fire size?\nAre no burn days effective in preventing human caused fires?\nCan we calculate what troops are the busiest?\nIs there a linear relationship between fire size and duration?\nAbout the dataset\nI am using the 2020 California Fire Perimeters data set available from gis.data.gov. A fire perimeter is the boundary of a fire measured by acres in this data set. This data set includes data from many different government agencies from different time periods. This data set includes prescribed burns and wildfire perimeters dating back to 1950 for CAL FIRE. Data from 2021 is not released until the fire season is over, which is on going at the time of this (Dec 18 2021). The United States Forest Service, USFS, has submitted records as far back as 1878 and the National Park Service, NPS, has submitted records from as far back as 1921. The types of fires included in this report has varied over the centuries which may add bias to the results. A couple important notes about the data is that from 1950 to 2001 it included USFS fires 10 acres and greater and CAL FIRE fires 300 acres and greater. BLM and NPS started inputting data since 2002 collecting fires 10 acres and greater. From 2002-2014 CAL FIRE expanded its criteria by including timber fires 10 acres or greater, brush fires 50 acres and greater, grass fires 300 acres and greater in size, wild land fires destroying 3 or more structures, and wild land fires causing 300,000 or more in damage. From 2014 and on the monetary requirement was dropped and the damage requirement is 3 or more habitable structures or commercial structures. In 1989 each unit was assigned to verify all 300 plus Acre fires from pre 1989 and as a result there is a statewide GIS layer from 1950-1999. Some errors that could occur when exploring this data is that duplicates may exist. For example, USFS and CAL FIRE could both capture the fire perimeter and submit it. In some cases they could even report different parameters of the same fire. While duplicate records is rare, there is an asterisk next to the cells that are the most accurate.\nExplanation of variables\nYEAR This set contains fire data from 1878 to 2020.\nSTATE While this data set is from CALFIRE there is some data from Oregon, Nevada, and Arizona. This data reports fires that occurred in both states, it does not specify the origin.\nAGENCY Different services may respond to the fires depending on jurisdiction. These services provide data to Cal Fire as a courtesy Different values we may see are CDF for California Department of Forestry and Fire Protection (Cal Fire), United States Forest Service (USFS), Bureau of Land Management (BLM), National Park Service (NPS), Contract Country (CC), Other FED (Federal Fire Protection Agency).\nUNIT_ID This is a series of digits to uniquely identify what units are responding to the fire.\nFIRE_NAME Fires are often named for geographic location or nearby landmarks like roads, lakes, rivers, and mountains.\nINC_NUM Number assigned by thee Emergency Command Center of the responsible agency for the fire.\nALARM_DATE The date the fire was brought to the attention of CALFIRE.\nCONT_DATE The date the fire was contained. A fire is 100% contained when a perimeter has been formed around the fire that will prevent it from spreading beyond the line. To form the perimeter fire fighters may use trenches (normally 10-12 feet and shallow), natural barriers like rivers, or even already burned patches of land. Once a fire is contained it may still be burning but within the perimeter.\nCAUSE An enumeration of values 1-19 for the reason the fire started. Enumeration 4 is campfire but enumeration 19 is illegal alien campfire which is confusing. Another confusing enumeration is 12 and 13 for firefighter training and non-firefighter training. Wouldn’t every fire that wasn’t firefighter training fall under this category of non-firefighter training. Interesting enumerations is 18 for escaped prescribed burn, 17 volcanic, 11 power line, 7 arson, 14 unknown, and 16 aircraft.\nCOMMENTS Miscellaneous comments that can provide more information about the fire.\nREPORT_AC Estimated area consumed in fire reported in acres.\nGIS_ACRES GIS is a geographic information system that uses information from satellites to make inferences. This is numerical data and the units are acres. Given the data is more complete for GIS_ACRES we will be working with this mostly.\nC_METHOD The method used to collect perimeter data. C stands for collection here. This is a range of digits from 1-8 that can be GPS, infrared, photos, hand drawn, or mixed collection methods.\nOBJECTIVE Either 1 suppression (wildfire) or 2 resource benefit (WFU). A WFU is allowing naturally ignited wild land fires like those started by lightning or lava to burn when in inaccessible terrain where people are not threatened. This is to avoid putting firefighters at risk and keep the land healthy.\nFIRE_NUM This has no description and is unclear at the moment. It is probably a method used to identify fires. There is not much research on it either, this will mostly be ignored.\nSHAPE_Length This is most likely GIS data. These map units are based on the coordinate system it could be square meters or something else. It could also be angular or linear.\nSHAPE_Area The units are unknown.\nDiscovery:\n\n[1] \"The fire dataset dimensions are (21318, 17)\"\n\n\n\nmissing_data <- c(1:ncol(fires))\nfor(col in 1:ncol(fires)) { \n  colname <- colnames(fires)[col]\n  nans <- sum(is.na(fires[, col]))\n  bad_strings <- c('', ' ', \"UNKNOWN\", \"UKNOWN\", \"N/A\")\n  bad_strings_count <- sum(fires[, col] %in% bad_strings)\n  zeros <- sum(fires[, col] <= 0)\n  total <- 0\n  if(!is.na(nans)) { \n    total <- nans \n  }\n  if(!is.na(bad_strings_count)) { \n    total <- total + bad_strings_count\n  }\n  if(!is.na(zeros)) { \n    total <- total + zeros\n  }\n  missing_data[col] <- total\n}\nmissing_data <- data.frame(colnames(fires), missing_data)\ncolnames(missing_data) <- c(\"Column\", \"Num NA's, '', or <= 0's\")\nmissing_data\n\n\n         Column Num NA's, '', or <= 0's\n1          year                      77\n2         state                       6\n3        agency                      10\n4       unit_id                      46\n5     fire_name                   13416\n6       inc_num                    1852\n7    alarm_date                   10728\n8     cont_date                   25360\n9         cause                      48\n10     comments                   36974\n11    report_ac                   12551\n12    gis_acres                       7\n13     c_method                   12222\n14    objective                     195\n15     fire_num                    8123\n16 shape_length                       0\n17   shape_area                   21317\n\nThere are many ways to deal with missing data like ignoring it or changing them to the mean or median. It is important to not change the integrity of the data if you manipulate missing data. The majority of the NA’s are in columns report_ac and c_method which means I will use shape_area instead. The rest of the values mostly come from missing values like ’’. This does not matter so much for comments but is a concern for inc_num, alarm_date, cont_data, and fire_num. For fire_name, many of the “UNKNOWN” fire names may stem from small prescribed burns. This is something to keep in mind as I use this data and will check to see if those old values originate from the older data or not.\nBrief summaries of every variable\nYEAR\n\n[1] \"The range of years in the dataset is from 1878 to 2020\"\n\n\nThis graph does not prove that more fires are occurring every year because we do not know how accurate reporting is in the early 1900s. CAL FIRE has data set going back to 1950 but USFS has data from 1878 in here. The two years with the most amount of data is 2017 and 2020 which has had the worse fires in recent history.\nSTATE\n\nTable 1: Number of observations for each state\nstates\nFreq\nAZ\n1\nCA\n21226\nNV\n73\nOR\n15\n\nNo fires that happened out of this state were under jurisdiction of Cal Fire which suggests these fires may have originated outside of California. These fires can be potentially misleading since the boundary can include both California and the other state. There is no way to determine what the boundary is for the California part of the fire.\nAGENCY\n\nTable 2: Number of reports by each agency\nagency\nnum_fires\nUSF\n9556\nCDF\n6563\nCCO\n3222\nNPS\n1011\nBLM\n652\nDOD\n163\nLRA\n111\nFWS\n22\nBIA\n12\nOTH\n1\n\nSurprisingly the United States Forest Service has been in charge of more fires than CalFire. However, this is likely due to them reporting many smaller prescribed burns. I wonder who is in charge of more land, and how jurisdiction is delegated. Private (PVT) was an option in the official documentation but there appears to be no occurrences in this data set. It seems like the groups to pay the most attention to are BLM, CCO, CDF, LRA, NPS, and USF.\n\n\n\n\n\n\nWe continue to see that the National Park Service has jurisdiction over more large fires than CAL Fire. Despite the increase of fires in recent years, the number of contracted counties in charge of fires has decreased. However, the Bureau of Land Management and National Park Service have been increasing their aid in combating California Fires.\nUNIT_ID\n\n\nout_of_state_units <- fires %>% \n  select(state, unit_id) %>% \n  filter(state != \"CA\" & state != \"\" & !is.na(state) & unit_id != '') %>% \n  unique()\nin_state_units <- fires %>% \n  select(state, unit_id) %>% \n  filter(state == \"CA\" & state != '' & !is.na(state) & unit_id != '') %>%\n  unique()\n#See if there are any units belonging to both states\ncross_state_units <- c() \nfor(row in 1:nrow(in_state_units)) { \n  unit <- in_state_units[row, \"unit_id\"]\n  if(sum(out_of_state_units[, \"unit_id\"] == unit) > 0) { \n    cross_state_units <- c(cross_state_units, unit)\n    }\n}\ncross_state_units_df <- fires %>% \n  filter(unit_id == cross_state_units & state != '' & !is.na(state)) %>% \n  select(state, agency, unit_id, gis_acres, fire_name) %>% \n  group_by(state, agency, unit_id) %>% \n  summarise(gis_acres_total = sum(gis_acres)) %>% \n  arrange(unit_id)\nkable(cross_state_units_df, caption = \n        \"The following units were responsible for fires in multiple states\")\n\n\nTable 3: The following units were responsible for fires in multiple states\nstate\nagency\nunit_id\ngis_acres_total\nCA\nBLM\nCCD\n1392.92076\nNV\nBLM\nCCD\n25.46643\nCA\nUSF\nHTF\n26751.67800\nNV\nUSF\nHTF\n14267.09300\nCA\nUSF\nKNF\n235463.13349\nOR\nUSF\nKNF\n476.97339\nCA\nBLM\nNOD\n18492.59674\nNV\nBLM\nNOD\n17.40460\nCA\nUSF\nTMU\n3092.50918\nCA\nUSF\nTOI\n18235.83105\nNV\nUSF\nTOI\n10366.60557\n\nThe reason that there are some units in charge of fires in multiple states is because they belong to federal organizations like United States Forest Service or Bureau of Land Management.\n\n\ncalfire_units_biggest_fire <- fires %>% \n  filter(agency == \"CDF\" & !is.na(gis_acres)) %>% \n  group_by(unit_id) %>% \n  slice(which.max(gis_acres)) %>% \n  select(year, agency, unit_id, fire_name, gis_acres) %>% \n  arrange(desc(gis_acres)) %>% \n  head(n=10)\nkable(calfire_units_biggest_fire, caption=\"CAL FIRE Units that Responded to the Top 10 Biggest Fires in California History\")\n\n\nTable 4: CAL FIRE Units that Responded to the Top 10 Biggest Fires in California History\nyear\nagency\nunit_id\nfire_name\ngis_acres\n2018\nCDF\nLNU\nRANCH\n410202.47\n2020\nCDF\nSCU\nSCU COMPLEX\n396399.00\n2007\nCDF\nSBC\nZACA\n240358.70\n2018\nCDF\nSHU\nCARR\n229651.41\n1977\nCDF\nBEU\nMARBLE-CONE\n173333.45\n2007\nCDF\nMVU\nWITCH\n162070.47\n2018\nCDF\nBTU\nCAMP\n153335.56\n1990\nCDF\nTGU\nCAMPBELL\n131504.22\n2020\nCDF\nCZU\nCZU LIGHTNING COMPLEX\n86553.46\n1985\nCDF\nSLU\nLAS PILITAS\n84271.42\n\nI would like to calculate the busiest years for each unit, however an accurate way of calculating this is stopping me. Factors to consider are center, spread, range, count, and max fires for each unit every year. I need a function that will return the hardest year based off this data.\nFIRE_NAME\n\n\n#There's still other bad fire names in the data set\nbad_fire_names <- c(\"UNKNOWN\", \"N/A\", \"\", \"UKNOWN\", \" \")\npopular_fire_names <- fires %>%\n  filter(!is.na(fire_name)) %>% \n  subset(!(fire_name %in% bad_fire_names)) %>% \n  group_by(fire_name) %>% \n  count(name=\"count\") %>% \n  arrange(desc(count)) %>% \n  head(n=10)\nkable(popular_fire_names, caption=\"10 Most Popular Names for Fires\")\n\n\nTable 5: 10 Most Popular Names for Fires\nfire_name\ncount\nCANYON\n45\nRIVER\n39\nLAKE\n37\nCREEK\n35\nCOTTONWOOD\n33\nBEAR\n32\nRIDGE\n30\nRANCH\n28\nSPRING\n28\nPINE\n27\n\n\n\nbiggest_fire_names <- fires %>% \n  arrange(desc(gis_acres)) %>% \n  select(year, fire_name, gis_acres) %>% \n  head(n=5)\nkable(biggest_fire_names, caption=\"Top 5 Biggest Fires in California History and their Names\")\n\n\nTable 6: Top 5 Biggest Fires in California History and their Names\nyear\nfire_name\ngis_acres\n2020\nAUGUST COMPLEX FIRES\n1032699.0\n2002\nBISCUIT\n501082.0\n2018\nRANCH\n410202.5\n2020\nSCU COMPLEX\n396399.0\n2020\nCREEK\n379882.2\n\n\n[1] \"A moment of condolences for anyone affected by the Camp Fire in Paradise or any other fire.\"\n  fire_name year unit_id gis_acres alarm_date  cont_date\n1      CAMP 2018     BTU  153335.6 2018-11-08 2018-11-26\n\nFire names are not unique and there are some fires that share the same name. There are many fire names that have been entered in incorrectly, that also contain back slashes or other gibberish.\nALARM_DATE\n\n\nnew_dates <- fires\nnew_dates$alarm_date <- as.Date(new_dates$alarm_date, \"%Y/%m/%d\")\nnew_dates$cont_date <- as.Date(new_dates$cont_date, \"%Y/%m/%d\")\nbusiest_days <- new_dates %>% \n  filter(!is.na(alarm_date)) %>% \n  group_by(alarm_date) %>% \n  summarise(num_fires = n(), \n            mean = mean(gis_acres), \n            median = median(gis_acres), \n            sd=sd(gis_acres)) %>% \n  arrange(desc(num_fires)) %>% \n  head(n=5)\nkable(busiest_days, caption=\"Top 5 Number of Daily Fires Since 1878\")\n\n\nTable 7: Top 5 Number of Daily Fires Since 1878\nalarm_date\nnum_fires\nmean\nmedian\nsd\n2008-06-21\n122\n5178.631\n245.33860\n16386.395\n1987-08-30\n61\n8144.967\n1932.24430\n13913.601\n2003-09-03\n56\n515.189\n58.68177\n1629.138\n2015-07-30\n50\n4250.587\n389.58737\n10438.913\n1999-08-23\n47\n7053.712\n1571.95800\n19095.666\n\nIt is hard to imagine that 122 fires are logged on June 6 even if it is a summer day. Before I analyze more time’s, lets find out the meaning behind this. One explanation is it could be a bunch of controlled burns. Many of these did share the same inc_number. However, many had different names and containment dates. While the primary cause on July 30, 2015 for lots of fires was smoking, on July 6, 2008 it was just lots of lightning.\nCONT_DATE\n\n\nformat <- \"%Y/%m/%d %H:%M:%S %z\"\nlongest_fires <- new_dates %>% \n  select(fire_name, alarm_date, cont_date, cause, objective) %>% \n  filter(!is.na(alarm_date) & !is.na(cont_date)) %>% \n  mutate(fire_duration = \n           difftime(cont_date, alarm_date, format, units=\"days\")) %>% \n  arrange(desc(fire_duration)) %>% \n  head(n=10)\nkable(longest_fires, caption=\"Longest Lasting Fires in California History Since 1878\")\n\n\nTable 8: Longest Lasting Fires in California History Since 1878\nfire_name\nalarm_date\ncont_date\ncause\nobjective\nfire_duration\nPIUTE\n1990-08-13\n1999-09-14\n1\n1\n3319 days\nRALSTON\n2006-09-05\n2007-09-17\n9\n1\n377 days\nWESTLAKE\n2006-07-08\n2007-07-09\n2\n1\n366 days\nLAVAL\n2012-06-03\n2013-06-03\n14\n1\n365 days\nGORGE\n2018-08-19\n2019-08-19\n14\n1\n365 days\nPANTHER\n2013-05-01\n2013-12-09\n9\n1\n222 days\nPOWERHOUSE\n2013-05-30\n2013-12-18\n9\n1\n202 days\nLAKE\n2015-06-17\n2015-12-31\n9\n1\n197 days\nROSASCO\n2019-05-23\n2019-12-02\n9\n1\n193 days\nPONY\n2016-06-07\n2016-12-15\n1\n1\n191 days\n\nIn the data set there are some alarm_dates and cont_dates entered incorrectly, like the cont_date occurring before the alarm_date. Hence, there is a possibility that these results have also been entered in incorrectly. The reason that some of these fires last so long, is that fire responders allow it, because it is in a rural area and fires can be beneficial. The cause for most of these fires is lightning, unknown, or miscellaneous.\n\n[1] “Fire Duration (Hours) Summary Statistics” Min. 1st Qu. Median Mean 3rd Qu. Max. 24 24 72 368 288 8760\n\nA problem is that there are so many missing values in cont_date there are very few fires you can find the fire duration for relative to the whole data set. Among those available, there is a huge variance in the duration of fires. These shorter lasting fires are not just prescribed burns and have many different causes.\nCAUSE\n\nTable 9: Causes of the Biggest Fires on Average since 1878\ncause\nstring_cause\navg_size\ncount\n4\nCampfire\n3530.47878\n380\n1\nLightning\n2969.34811\n3454\n16\nAircraft\n2474.22459\n14\n9\nMiscellaneous\n2285.43451\n3379\n7\nArson\n2200.11588\n903\n10\nVehicle\n1534.41743\n454\n14\nUnknown/Unidentified\n1496.36036\n9541\n2\nEquipment Use\n1460.92934\n1246\n11\nPower Line\n1274.56765\n412\n6\nRailroad\n1169.29763\n80\n3\nSmoking\n1048.65940\n342\n13\nNon-Firefighter Training\n1013.68027\n11\n5\nDebris\n875.22631\n723\n8\nPlaying with Fire\n657.54545\n196\n18\nEscaped Prescribed Burn\n623.48734\n90\n12\nFirefighter Training\n594.01999\n5\n15\nStructure\n391.06574\n21\n19\nIllegal Alien Campfire\n83.46374\n17\n\nAs I previously mentioned, the reason some fires duration is so long is because they are in remote areas and allowed to continue to burn. Despite this, human induced fires like campfires, still produce bigger fires on average than nature induced fires like lightning. In the next sections we will explore if certain causes of fires are becoming more or less frequent.\nCOMMENTS\nThe max length of a comment is 260 characters, comments are longer but they are cut off by an * indicating there is more to that comment somewhere. By reading comments interesting ones to me were “The cause was target shooting”, “… Total Cost 18,600,600”, “Children playing with fire”, and names of people. Most of the comments in the data set are empty.\nGIS_ACRES\n\nTable 10: Highest Acres Burnage in California History since 1878\nyear\ngis_acres\nsquare_miles\n2020\n4159334.0\n6498.959\n2018\n1590431.2\n2485.049\n2017\n1424559.4\n2225.874\n2008\n1382462.2\n2160.097\n2007\n1040224.3\n1625.350\n2003\n970479.3\n1516.374\n2002\n963898.5\n1506.091\n1987\n862910.9\n1348.298\n2012\n847714.6\n1324.554\n1999\n801137.0\n1251.777\n\nFor reference 500,000 acres is equal to 780 square miles, a square mile being a square with each side being 1 mile in length!!! The size of Yosemite national park is 1169 square miles, and the size of New York is 302 square miles.\n\n[1] \"Number of fire reports where the difference between report_ac and gis_acres is greater than 100: 1250\"\n\nThere are many differences between REPORT_AC and GIS_ACRES. Many of the discrepancies come from the 2000s as well so it is not the fault of the older data. There is less missing data for GIS_ACRES so we will primarily be depending on this. There is the possibility of using REPORT_AC data if GIS_ACRES is missing and vice versa.\nC_METHOD\n\n\nget_mode <- function(vec) { \n  frequencies <- table(vec) %>% sort(decreasing=TRUE)\n  strtoi(names(frequencies)[1])\n} \n\nc_method_yr <- fires %>% \n  filter(!is.na(c_method) & c_method != 8) %>%\n  group_by(year) %>%\n  summarise(c_method = get_mode(c_method))\nyear <- fires %>% filter(year == \"2020\")\n\n\n\n\n\n#Collection methods \nfirst_yr_c_method <- fires %>% select(year, c_method) %>% \n  drop_na() %>% arrange(year) %>% head()\ntotal_c <- table(fires$c_method)\nlabels <- c(\"GPS Ground\", \"GPS Air\", \"Infared\", \"Other Imagery\", \n            \"Photo Interpretation\", \"Hand Drawn\", \"Mixed Collection Tools\",\n            \"Unknown\")\ndf <- data.frame(Method = labels, count = as.vector(total_c))\ndf <- arrange(df, desc(count))\n\n\n\n\n\n\n\n\n\nHere we can see the transition of the most frequently used collection method as time goes on.\nOBJECTIVE\n\nTable 11: Top 5 Biggest Wildfires in California since 1878\nyear\nfire_name\ngis_acres\ncause\nobjective\n2020\nAUGUST COMPLEX FIRES\n1032699.0\n1\n1\n2002\nBISCUIT\n501082.0\n1\n1\n2018\nRANCH\n410202.5\n14\n1\n2020\nSCU COMPLEX\n396399.0\n1\n1\n2020\nCREEK\n379882.2\n14\n1\nTable 11: Top 5 Biggest Prescribed Burns since 1878\nyear\nfire_name\ngis_acres\ncause\nobjective\n2008\nCLOVER\n15788.590\n1\n2\n2008\nTEHIPITE\n11648.092\n1\n2\n1977\nFERGUSON\n10421.790\n1\n2\n2005\nCOMB\n9756.467\n1\n2\n2010\nSHEEP COMPLEX\n9021.522\n1\n2\n\nAs we saw previously with the longest lasting fires in recent history most of them originated from wildfires. It is interesting that the cause of the biggest prescribed fires have all been lightning. In fact, there are no prescribed burns that do not originate from lightning in this data set that has GIS_ACRES data.\n\n\n#Are number of prescribed burns increasing? \nyearly_resource_burns <- resource %>% \n  filter(!is.na(gis_acres) & gis_acres > 0) %>% \n  select(year, gis_acres) %>% \n  group_by(year) %>% \n  summarise(num_fires = n(), total_acres_burned = sum(gis_acres)) \n\n\n\n\n\n\nThe number of acres burned in prescribed burns has never been lower until this last decade. At the same time, California has experienced some of the worst fires in the last decade. While correlation does not prove causation, poor fire management in the off season may explain the recent surge in fires.\nExploratory Data Analysis\n\n\n# Number of acres burned 2010-2020\ndecade_fires <- fires %>% select(year, gis_acres) %>%\n  filter(year >= \"2010\" & !is.na(gis_acres)) \ndecade_fires_sum <- decade_fires %>% group_by(year) %>% \n  summarise(gis_acres = sum(gis_acres))\ndecade_fires_sum$gis_acres <- as.integer(decade_fires_sum$gis_acres)\nggplot(decade_fires_sum, aes(x=year, y=gis_acres)) + \n  geom_bar(stat=\"identity\", fill=\"gray70\") + \n  geom_text(aes(label=gis_acres)) + \n  xlab(\"Years\") + ylab(\"Acres Burned\") + \n  ggtitle(\"Number of Acres Burned 2010-2020\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArson does not have much statistical analysis, it is just surprising how much land is burned every year due to this crime.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThe biggest discovery is that the number of prescribed burns has never been lower from 2010-2020 in recent California Fire History. This may be cause for why we have seen such a big spike in fires in recent history. Also, R’s ggplot makes it extremely difficult to customize your graph.\n\n\n\n",
    "preview": "posts/2022-03-02-firehistory/firehistory_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-scout/",
    "title": "Scout",
    "description": "A cross platform mobile app for iOS and Android devices. This project was built using the Flutter framework by Google. This is an application for ordering girl scout cookies and gives girl scouts a dashboad to control their inventory.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-03-02",
    "categories": [],
    "contents": "\nLaunch Screen\nScout logoCookie Menu\n A menu to browse cookies and add to your cart. When you are at the cart, you are can return to the menu and edit your order. Once you are finished with your order you are taken to a form to fill out cookie order information.\nOrder Form\n Information is verified to be correct like a valid phone number and email address. A drop down menu is expanded to select the proper state and only orders from the USA are currently accepted.\nDatabase\n This page fetches data from the database so the scout can view order information for their customers.\nStats\n Data visualizations to see trends in cookie sales and track inventory.\nEvents\n Scouts have the ability to create events so people can be notified when and where they are selling cookies.\n\n\n\n",
    "preview": "posts/2022-03-02-scout/menu.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-01-ios-dev/",
    "title": "iOS development",
    "description": "Swift is an open source programming language Apple uses for iOS, macOS, tvOS, and other devices. These examples are with UIKit a more mature and compatible framework than SwiftUI at the time of this writing (2022). This is programatically done in UIKit without the use of Storyboard. This approach was taken so that the app would run more efficiently and there would be more control over existing frameworks.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\nUsing Apple Maps\n The application incorporates Apple’s most up to date privacy guidelines. Certain features are available depending on the user’s comfort level providing location data. If the user allows location, the map will focus on their current location.\nLocation Search Controller\n The user can enter in a location to annotate the map. The search controller uses data from Apple’s MapKit to respond with real live locations data like address, lng/lat, and place name. The location data cheats and looks for locations nearest you.\nUIButton\n This is a password protected application for encrypting information on your device. Once unlocked with your password the user has a UICollectionView to store data.\nProgramming the constraints\n Proper construction of constraints allows your application to scale to smaller devices like the iPhone 4 and bigger devices like the iPhone 10. The code for specifying the constraints for these 9 buttons is much more efficient than using storyboard for all 9 UIButtons.\nUITableViewController\n A UITableViewController is one of the most frequently used ViewControllers alongise the ever more popular UICollectionView. This is a scrollable feed where user’s can see their friends posts.\nProgramming a UITableViewController\n It is not that much code to programatically create a UITableViewController. However, there is another class for configuring the cell. The UITableViewCell class is a template that contains a UIImage and UITextView, but can be customized further.\n\n\n\n",
    "preview": "posts/2022-01-01-ios-dev/swift_images/wrongpassword.png",
    "last_modified": "2022-03-13T21:59:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-01-find-the-giraffe/",
    "title": "Find the Giraffe",
    "description": "Refresh the page to play again.",
    "author": [
      {
        "name": "Tommy Smale",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-05-24T22:23:25-07:00",
    "input_file": {}
  }
]
